{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will start again our work to get topics from posts.  \n",
    "The reason of this change is simple : \n",
    "* There are a lot of pre-processing actions we could do to improve our initial dataset (lemmatization, group words together, filter words,...)\n",
    "* Our LDA model we used before got us an error we couldn't fix. This time, based on a new course (https://www.youtube.com/watch?v=6zm9NC9uRkk&ab_channel=PyData), we will improve our code.\n",
    "\n",
    "Let's begin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>#Reactions</th>\n",
       "      <th>#Comments</th>\n",
       "      <th>Location</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Time_spent</th>\n",
       "      <th>Media_type</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nicholas Wyman</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6484.0</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>article</td>\n",
       "      <td>robert lerman  writes that achieving a healthy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nicholas Wyman</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6484.0</td>\n",
       "      <td>1 week ago</td>\n",
       "      <td>none</td>\n",
       "      <td>national disability advocate  sara hart weir, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nicholas Wyman</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6484.0</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>article</td>\n",
       "      <td>exploring in this months talent management &amp; h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nicholas Wyman</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6484.0</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>article</td>\n",
       "      <td>i count myself fortunate to have spent time wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nicholas Wyman</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>6484.0</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>article</td>\n",
       "      <td>online job platforms are a different way of wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34007</th>\n",
       "      <td>Simon Sinek</td>\n",
       "      <td>4005</td>\n",
       "      <td>93</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4206024.0</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>image</td>\n",
       "      <td>igniter of the year 2016. well i know that i'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34008</th>\n",
       "      <td>Simon Sinek</td>\n",
       "      <td>1698</td>\n",
       "      <td>74</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4206024.0</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>video</td>\n",
       "      <td>executives who prioritize the shareholder are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34009</th>\n",
       "      <td>Simon Sinek</td>\n",
       "      <td>661</td>\n",
       "      <td>59</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4206024.0</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>video</td>\n",
       "      <td>like many, i too have been reflecting as we ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34010</th>\n",
       "      <td>Simon Sinek</td>\n",
       "      <td>766</td>\n",
       "      <td>35</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4206024.0</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>video</td>\n",
       "      <td>if you say \"customer first\" that means your em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34011</th>\n",
       "      <td>Simon Sinek</td>\n",
       "      <td>789</td>\n",
       "      <td>23</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>4206024.0</td>\n",
       "      <td>4 years ago</td>\n",
       "      <td>none</td>\n",
       "      <td>the small work hard to serve themselves in a b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31996 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name  #Reactions  #Comments Location  Followers  \\\n",
       "0      Nicholas Wyman          12          1  Unknown     6484.0   \n",
       "1      Nicholas Wyman          11          0  Unknown     6484.0   \n",
       "3      Nicholas Wyman          44          0  Unknown     6484.0   \n",
       "4      Nicholas Wyman          22          2  Unknown     6484.0   \n",
       "5      Nicholas Wyman          21          1  Unknown     6484.0   \n",
       "...               ...         ...        ...      ...        ...   \n",
       "34007     Simon Sinek        4005         93  Unknown  4206024.0   \n",
       "34008     Simon Sinek        1698         74  Unknown  4206024.0   \n",
       "34009     Simon Sinek         661         59  Unknown  4206024.0   \n",
       "34010     Simon Sinek         766         35  Unknown  4206024.0   \n",
       "34011     Simon Sinek         789         23  Unknown  4206024.0   \n",
       "\n",
       "         Time_spent Media_type  \\\n",
       "0         1 day ago    article   \n",
       "1        1 week ago       none   \n",
       "3      2 months ago    article   \n",
       "4      2 months ago    article   \n",
       "5      2 months ago    article   \n",
       "...             ...        ...   \n",
       "34007   4 years ago      image   \n",
       "34008   4 years ago      video   \n",
       "34009   4 years ago      video   \n",
       "34010   4 years ago      video   \n",
       "34011   4 years ago       none   \n",
       "\n",
       "                                                 Content  \n",
       "0      robert lerman  writes that achieving a healthy...  \n",
       "1      national disability advocate  sara hart weir, ...  \n",
       "3      exploring in this months talent management & h...  \n",
       "4      i count myself fortunate to have spent time wi...  \n",
       "5      online job platforms are a different way of wo...  \n",
       "...                                                  ...  \n",
       "34007  igniter of the year 2016. well i know that i'm...  \n",
       "34008  executives who prioritize the shareholder are ...  \n",
       "34009  like many, i too have been reflecting as we ne...  \n",
       "34010  if you say \"customer first\" that means your em...  \n",
       "34011  the small work hard to serve themselves in a b...  \n",
       "\n",
       "[31996 rows x 8 columns]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_pickle(\"contentCorpus.pkl\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's devide this dataset into two datasets : Corpus only & Informations about the post\n",
    "\n",
    "# We create a specific ID for each row\n",
    "data[\"ID\"]=range(data.shape[0])\n",
    "\n",
    "# We create the dataset containing content\n",
    "corpus = data[['ID','Content']]\n",
    "\n",
    "# And the one containing reactions & comments about each post\n",
    "# We don't need other columns for this analysis\n",
    "postReactions = data[['ID','#Reactions','#Comments']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national disability advocate  sara hart weir, ms   shares how congress passed the able act\n",
      "--------------\n",
      "for those intersted in youth career pathways. great to read today about the expansion of citi foundation’s pathways to progress inititiave - new commitment to 500k young adults #jobready #jobs #training  joanne gedge   janet searle   louise martin lindsay   amy-lou cowdroy-ling   https://lnkd.in/g8ftr5w. \n",
      "--------------\n",
      "community building has meant something dramatically different the past couple of months.  when 500 startups hosted an event with 2400+ rsvps, we had to pivot almost 8 times to accommodate changing restrictions along the way.  people are rallying around their communities to show support for groups like healthcare workers while staying in their homes for months.  all these things have made me ask the question, what's the best way to build community right now? how is this changing our perception of community? thus, i'm starting my letter to the community on the subject. \n",
      "--------------\n",
      "where can we find  #casestudies  of  #startups  that were built by a  #team  of  #entrepreneurs  virtually?\n",
      "--------------\n",
      "i'm back in the u.s., inspired, enthused and invigorated. so as i sit in the atlanta airport awaiting one more flight, i’m reviewing my pre-trip to-do list: explored my 12th country on the mamaland? check. interviewed a #maternalhealth superhero and living legend? check. renewed my love affair with #africa? triple check. and now that i’m back in the land of stable internet, i’ll be posting more highlights and insights from my journey on  instagram  at rachella55, and here in  linkedin .  btw, thanks so much, linkedin fam, for all the kind support and compliments you shared about my posts while i was traveling. you keep me inspired, too! #natgeotraveller #globalhealth \n",
      "--------------\n"
     ]
    }
   ],
   "source": [
    "for c in corpus['Content'][[1,100,1000,10000,20000]]:\n",
    "    print(c)\n",
    "    print('--------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, some words can be filtered :\n",
    "* mentions (joanne gedge, janet searle,...)\n",
    "* some hashtags that are involved in sentences (natgeotraveller , globalhealth), generally at the end of the post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts Preprocessing : what we can do ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this preprocessing, we will use Spacy, which is a fast industrial-strength natural language processing (NLP) library for Python.\n",
    "\n",
    "spaCy handles many tasks commonly associated with building an end-to-end natural language processing pipeline:\n",
    "\n",
    "* Tokenization\n",
    "* Text normalization, such as lowercasing, stemming/lemmatization\n",
    "* Part-of-speech tagging\n",
    "* Syntactic dependency parsing\n",
    "* Sentence boundary detection\n",
    "* Named entity recognition and annotation\n",
    "\n",
    "In the \"batteries included\" Python tradition, spaCy contains built-in data and models which you can use out-of-the-box for processing general-purpose English language text:\n",
    "\n",
    "* Large English vocabulary, including stopword lists\n",
    "* Token \"probabilities\"\n",
    "* Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import codecs\n",
    "\n",
    "# We need to import spacy trained pipelines, which support many languages\n",
    "# Let's use the English pipeline\n",
    "\n",
    "# In CMD :\n",
    "# $ python -m spacy download en_core_web_lg\n",
    "# or\n",
    "# In python :\n",
    "# >>> import spacy\n",
    "# >>> nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# We download the large version \"lg\" and not the small version \"sm\" here \n",
    "# This module is pretty large : ~631 MB\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a sample of several posts to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national disability advocate  sara hart weir, ms   shares how congress passed the able act\n",
      "\n",
      "for those intersted in youth career pathways. great to read today about the expansion of citi foundation’s pathways to progress inititiave - new commitment to 500k young adults #jobready #jobs #training  joanne gedge   janet searle   louise martin lindsay   amy-lou cowdroy-ling   https://lnkd.in/g8ftr5w. \n",
      "\n",
      "community building has meant something dramatically different the past couple of months.  when 500 startups hosted an event with 2400+ rsvps, we had to pivot almost 8 times to accommodate changing restrictions along the way.  people are rallying around their communities to show support for groups like healthcare workers while staying in their homes for months.  all these things have made me ask the question, what's the best way to build community right now? how is this changing our perception of community? thus, i'm starting my letter to the community on the subject. \n",
      "\n",
      "where can we find  #casestudies  of  #startups  that were built by a  #team  of  #entrepreneurs  virtually?\n",
      "\n",
      "i'm back in the u.s., inspired, enthused and invigorated. so as i sit in the atlanta airport awaiting one more flight, i’m reviewing my pre-trip to-do list: explored my 12th country on the mamaland? check. interviewed a #maternalhealth superhero and living legend? check. renewed my love affair with #africa? triple check. and now that i’m back in the land of stable internet, i’ll be posting more highlights and insights from my journey on  instagram  at rachella55, and here in  linkedin .  btw, thanks so much, linkedin fam, for all the kind support and compliments you shared about my posts while i was traveling. you keep me inspired, too! #natgeotraveller #globalhealth \n"
     ]
    }
   ],
   "source": [
    "#Posts are separated by line breaks, and gather into one string\n",
    "posts_sample = \"\\n\\n\".join(corpus.Content[[1,100,1000,10000,20000]])\n",
    "\n",
    "print(posts_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national disability advocate  sara hart weir, ms   shares how congress passed the able act\n",
      "\n",
      "for those intersted in youth career pathways. great to read today about the expansion of citi foundation’s pathways to progress inititiave - new commitment to 500k young adults #jobready #jobs #training  joanne gedge   janet searle   louise martin lindsay   amy-lou cowdroy-ling   https://lnkd.in/g8ftr5w. \n",
      "\n",
      "community building has meant something dramatically different the past couple of months.  when 500 startups hosted an event with 2400+ rsvps, we had to pivot almost 8 times to accommodate changing restrictions along the way.  people are rallying around their communities to show support for groups like healthcare workers while staying in their homes for months.  all these things have made me ask the question, what's the best way to build community right now? how is this changing our perception of community? thus, i'm starting my letter to the community on the subject. \n",
      "\n",
      "where can we find  #casestudies  of  #startups  that were built by a  #team  of  #entrepreneurs  virtually?\n",
      "\n",
      "i'm back in the u.s., inspired, enthused and invigorated. so as i sit in the atlanta airport awaiting one more flight, i’m reviewing my pre-trip to-do list: explored my 12th country on the mamaland? check. interviewed a #maternalhealth superhero and living legend? check. renewed my love affair with #africa? triple check. and now that i’m back in the land of stable internet, i’ll be posting more highlights and insights from my journey on  instagram  at rachella55, and here in  linkedin .  btw, thanks so much, linkedin fam, for all the kind support and compliments you shared about my posts while i was traveling. you keep me inspired, too! #natgeotraveller #globalhealth \n"
     ]
    }
   ],
   "source": [
    "parsed_posts_sample = nlp(posts_sample)\n",
    "\n",
    "print(parsed_posts_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks the same ! So what happened ?  \n",
    "Let's apply some functions !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, we have already removed punctuation, but it is working very well !!!  \n",
    "I keep that here for a later use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "national disability advocate  sara hart weir, ms   shares how congress passed the able act\n",
      "\n",
      "for those intersted in youth career pathways.\n",
      "\n",
      "Sentence 2\n",
      "great to read today about the expansion of citi foundation’s pathways to progress inititiave - new commitment to 500k young adults #jobready #jobs #training  joanne gedge   janet searle   louise martin lindsay   amy-lou cowdroy-ling   https://lnkd.in/g8ftr5w.\n",
      "\n",
      "Sentence 3\n",
      "\n",
      "\n",
      "community building has meant something dramatically different the past couple of months.\n",
      "\n",
      "Sentence 4\n",
      " when 500 startups hosted an event with 2400+ rsvps, we had to pivot almost 8 times to accommodate changing restrictions along the way.\n",
      "\n",
      "Sentence 5\n",
      " people are rallying around their communities to show support for groups like healthcare workers while staying in their homes for months.\n",
      "\n",
      "Sentence 6\n",
      " all these things have made me ask the question, what's the best way to build community right now?\n",
      "\n",
      "Sentence 7\n",
      "how is this changing our perception of community?\n",
      "\n",
      "Sentence 8\n",
      "thus, i'm starting my letter to the community on the subject.\n",
      "\n",
      "Sentence 9\n",
      "\n",
      "\n",
      "where can we find  #casestudies  of  #startups  that were built by a  #team  of  #entrepreneurs  virtually?\n",
      "\n",
      "\n",
      "\n",
      "Sentence 10\n",
      "i'm back in the u.s., inspired, enthused and invigorated.\n",
      "\n",
      "Sentence 11\n",
      "so as i sit in the atlanta airport awaiting one more flight, i’m reviewing my pre-trip to-do list: explored my 12th country on the mamaland?\n",
      "\n",
      "Sentence 12\n",
      "check.\n",
      "\n",
      "Sentence 13\n",
      "interviewed a #maternalhealth superhero and living legend?\n",
      "\n",
      "Sentence 14\n",
      "check.\n",
      "\n",
      "Sentence 15\n",
      "renewed my love affair with #africa?\n",
      "\n",
      "Sentence 16\n",
      "triple check.\n",
      "\n",
      "Sentence 17\n",
      "and now that i’m back in the land of stable internet, i’ll be posting more highlights and insights from my journey on  instagram  at rachella55, and here in  linkedin .\n",
      "\n",
      "Sentence 18\n",
      " btw, thanks so much, linkedin fam, for all the kind support and compliments you shared about my posts while i was traveling.\n",
      "\n",
      "Sentence 19\n",
      "you keep me inspired, too!\n",
      "\n",
      "Sentence 20\n",
      "#natgeotraveller #globalhealth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_posts_sample.sents):\n",
    "    print(\"Sentence {}\".format(num+1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "For those intersted in youth career pathways.\n",
      "\n",
      "Sentence 2\n",
      "Great to read today about the expansion of Citi Foundation’s Pathways to Progress inititiave - new commitment to 500k young adults #JobReady #Jobs #Training  Joanne Gedge   Janet Searle   Louise Martin Lindsay   Amy-Lou Cowdroy-Ling   https://lnkd.in/g8FTr5w.\n",
      "\n",
      "Sentence 3\n",
      "\n",
      " \n",
      " \n",
      " …see more\n",
      "\n",
      "Community building has meant something dramatically different the past couple of months.\n",
      "\n",
      "Sentence 4\n",
      " When 500 Startups hosted an event with 2400+ RSVPs, we had to pivot almost 8 times to accommodate changing restrictions along the way.\n",
      "\n",
      "Sentence 5\n",
      " People are rallying around their communities to show support for groups like healthcare workers while staying in their homes for months.\n",
      "\n",
      "Sentence 6\n",
      " All these things have made me ask the question, what's the best way to build community right now?\n",
      "\n",
      "Sentence 7\n",
      "How is this changing our perception of community?\n",
      "\n",
      "Sentence 8\n",
      "Thus, I'm starting my letter to the community on the subject.\n",
      "\n",
      "Sentence 9\n",
      "\n",
      " \n",
      " \n",
      " …see more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Same with the original content, containing ponctuations & uppercases\n",
    "test = pd.read_pickle(\"cleaned_data.pkl\")\n",
    "test_content = test.content\n",
    "sample_test_content = \"\\n\\n\".join(test_content[[100,1000]])\n",
    "parsed_sample_test = nlp(sample_test_content)\n",
    "\n",
    "for num, sentence in enumerate(parsed_sample_test.sents):\n",
    "    print(\"Sentence {}\".format(num+1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name entity detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1 congress - ORG\n",
      "Entity 2 today - DATE\n",
      "Entity 3 citi foundation’s - ORG\n",
      "Entity 4 500k - ORG\n",
      "Entity 5 joanne gedge   janet searle   louise martin lindsay   amy-lou cowdroy-ling - PERSON\n",
      "Entity 6 the past couple of months - DATE\n",
      "Entity 7 500 - CARDINAL\n",
      "Entity 8 2400 - CARDINAL\n",
      "Entity 9 8 - CARDINAL\n",
      "Entity 10 months - DATE\n",
      "Entity 11 u.s. - GPE\n",
      "Entity 12 atlanta - GPE\n",
      "Entity 13 one - CARDINAL\n",
      "Entity 14 12th - ORDINAL\n",
      "Entity 15 linkedin fam - PERSON\n"
     ]
    }
   ],
   "source": [
    "for num, entity in enumerate(parsed_posts_sample.ents):\n",
    "    print(\"Entity {}\".format(num+1), entity, '-', entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ORG : organism \n",
    "* GPE : geopolitcal entity\n",
    "* LOC : location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we can know which entity represents each words or group of words ! \n",
    "\n",
    "For instance, spacy knows when a group of words is a name, a location, a date ...  \n",
    "That's clearly amazing !\n",
    "\n",
    "Also, it appears that some words are badly comprehend by the algorithm like \"us\", as USA, which is here \"us\" designating we.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define if a word is an adjective, a noun, or other..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token_text</th>\n",
       "      <th>Token_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disability</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advocate</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>SPACE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sara</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>!</td>\n",
       "      <td>PUNCT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>#</td>\n",
       "      <td>SYM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>natgeotraveller</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>#</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>globalhealth</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Token_text Token_pos\n",
       "0           national       ADJ\n",
       "1         disability      NOUN\n",
       "2           advocate      NOUN\n",
       "3                        SPACE\n",
       "4               sara     PROPN\n",
       "..               ...       ...\n",
       "355                !     PUNCT\n",
       "356                #       SYM\n",
       "357  natgeotraveller      NOUN\n",
       "358                #      NOUN\n",
       "359     globalhealth      NOUN\n",
       "\n",
       "[360 rows x 2 columns]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_text = [token.orth_ for token in parsed_posts_sample]\n",
    "token_pos = [token.pos_ for token in parsed_posts_sample]\n",
    "\n",
    "pd.DataFrame({\"Token_text\" : token_text , \"Token_pos\" : token_pos})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalization (stemming/lemmatization and shape analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization consists in transforming a word into its root. \n",
    "\n",
    "For instance, \"is\" becomes \"be\" ; \"me\" becomes \"I\" ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Token_text</th>\n",
       "      <th>Token_lemma</th>\n",
       "      <th>Token_shape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national</td>\n",
       "      <td>national</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disability</td>\n",
       "      <td>disability</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advocate</td>\n",
       "      <td>advocate</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sara</td>\n",
       "      <td>sara</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "      <td>!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>natgeotraveller</td>\n",
       "      <td>natgeotraveller</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "      <td>#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>globalhealth</td>\n",
       "      <td>globalhealth</td>\n",
       "      <td>xxxx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Token_text      Token_lemma Token_shape\n",
       "0           national         national        xxxx\n",
       "1         disability       disability        xxxx\n",
       "2           advocate         advocate        xxxx\n",
       "3                                                \n",
       "4               sara             sara        xxxx\n",
       "..               ...              ...         ...\n",
       "355                !                !           !\n",
       "356                #                #           #\n",
       "357  natgeotraveller  natgeotraveller        xxxx\n",
       "358                #                #           #\n",
       "359     globalhealth     globalhealth        xxxx\n",
       "\n",
       "[360 rows x 3 columns]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemma = [token.lemma_ for token in parsed_posts_sample]\n",
    "token_shape = [token.shape_ for token in parsed_posts_sample]\n",
    "\n",
    "pd.DataFrame({'Token_text' : token_text, 'Token_lemma' : token_lemma , 'Token_shape' : token_shape})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about a variety of other token-level attributes, such as the relative frequency of tokens, and whether or not a token matches any of these categories?\n",
    "\n",
    "* stopword\n",
    "* punctuation\n",
    "* whitespace\n",
    "* represents a number\n",
    "* whether or not the token is included in spaCy's default vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop?</th>\n",
       "      <th>punctuation?</th>\n",
       "      <th>whitespace?</th>\n",
       "      <th>number?</th>\n",
       "      <th>out of vocab.?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>disability</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>advocate</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sara</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>!</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>#</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>natgeotraveller</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>#</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>globalhealth</td>\n",
       "      <td>-20.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                text  log_probability stop? punctuation? whitespace? number?  \\\n",
       "0           national            -20.0                                          \n",
       "1         disability            -20.0                                          \n",
       "2           advocate            -20.0                                          \n",
       "3                               -20.0                            Yes           \n",
       "4               sara            -20.0                                          \n",
       "..               ...              ...   ...          ...         ...     ...   \n",
       "355                !            -20.0                Yes                       \n",
       "356                #            -20.0                Yes                       \n",
       "357  natgeotraveller            -20.0                                          \n",
       "358                #            -20.0                Yes                       \n",
       "359     globalhealth            -20.0                                          \n",
       "\n",
       "    out of vocab.?  \n",
       "0                   \n",
       "1                   \n",
       "2                   \n",
       "3              Yes  \n",
       "4                   \n",
       "..             ...  \n",
       "355                 \n",
       "356                 \n",
       "357            Yes  \n",
       "358                 \n",
       "359            Yes  \n",
       "\n",
       "[360 rows x 7 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attributes = [(token.orth_,\n",
    "                     token.prob,\n",
    "                     token.is_stop,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.like_num,\n",
    "                     token.is_oov)\n",
    "                    for token in parsed_posts_sample]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                  columns=['text',\n",
    "                           'log_probability',\n",
    "                           'stop?',\n",
    "                           'punctuation?',\n",
    "                           'whitespace?',\n",
    "                           'number?',\n",
    "                           'out of vocab.?'])\n",
    "\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: u'Yes' if x else u''))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Log_probability represents the frequency of a word apparation in the text : \n",
    "    *  ~ 0 if appears often\n",
    "    * =! 0 if appears rarely \n",
    "    \n",
    "\n",
    "* Stop ? : Is this word a stop word ?\n",
    "\n",
    "* Out of vocab ? : Is this word out of the english dictionary proposed by Spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP preprocessing : application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will clean a little bit the content by removing hashtags. Indeed, it appears that some hastags have no sense for future models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned !:\n",
      "----------\n",
      "robert lerman  writes that achieving a healthy future of work requires employees to build skills that help them attain productive and rewarding careers. he notes - \"one of the most cost-effective ways to do this is through apprenticeship, which helps workers master occupations and gain professional identity and pride\". coudlnt agree more!        read the article on    urban institute \n",
      "\n",
      "national disability advocate  sara hart weir, ms   shares how congress passed the able act\n",
      "\n",
      "exploring in this months talent management & hr what a company should consider to get the most out of a modern apprenticeship program. thanks to employer & entrepreneuer  ankur gopal  for sharing insights on your it program.  why not start a program in 2021.. wishing you all a safe and happy festive season.  nick          urban institute   zach boren   robert lerman   lana gordon   andrew sezonov   simon w.   ervin dimeny \n",
      "\n",
      "i count myself fortunate to have spent time with brooklyn-born arnold packer. arnold was the assistant secretary for policy, evaluation, and research in the u.s. department of labor during the carter administration. his insights and innovative thinking around economics, employment and training policies helped many people, particularly young people to reach their full potential.  arnold passed away aged 85 in mid-october. the last project we were working up with my colleage  ervin dimeny  was around verified resumes. thanks to the  urban institute  report by  robert lerman   daniel kuehn  and  pamela loprest   - wonderful insights into the topic.  love to hear your thoughts on   - join the conversation. stay safe.    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's create a function to apply to the pd.Serie\n",
    "def rm_hashtags(str):\n",
    "    return \" \".join(word for word in str.split(' ') if \"#\" not in word)\n",
    "\n",
    "first_clean_corpus = corpus['Content'].apply(rm_hashtags)\n",
    "\n",
    "print(\"Cleaned !:\")\n",
    "print(\"----------\")\n",
    "\n",
    "for post in first_clean_corpus[:4]:\n",
    "    print(post)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrase modeling is another approach to learning combinations of tokens that together represent meaningful multi-word concepts. We can develop phrase models by looping over the the words in our reviews and looking for words that co-occur (i.e., appear one after another) together much more frequently than you would expect them to by random chance. The formula our phrase models will use to determine whether two tokens  and  constitute a phrase is:\n",
    "\n",
    "$$ \\frac{count (AB) - count_{min}}{count(A)*count(B)} * N > treshold $$\n",
    "\n",
    " \n",
    "...where:\n",
    "\n",
    " * $count(A)$ is the number of times token $A$ appears in the corpus\n",
    " * $count(B)$ is the number of times token $B$ appears in the corpus\n",
    " * $count(AB)$ is the number of times the tokens $AB$ appear in the corpus in order\n",
    " * $N$ is the total size of the corpus vocabulary\n",
    " * $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times\n",
    " * $treshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "Once our phrase model has been trained on our corpus, we can apply it to new text. When our model encounters two tokens in new text that identifies as a phrase, it will merge the two into a single new token.\n",
    "\n",
    "Phrase modeling is superficially similar to named entity detection in that you would expect named entities to become phrases in the model (so new york would become new_york). But you would also expect multi-word expressions that represent common concepts, but aren't specifically named entities (such as happy hour) to also become phrases in the model.\n",
    "\n",
    "We turn to the indispensible **gensim** library to help us with phrase modeling — the Phrases class in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're performing phrase modeling, we'll be doing some iterative data transformation at the same time. Our roadmap for data preparation includes:\n",
    "\n",
    "* Segment text of complete reviews into sentences & normalize text\n",
    "* First-order phrase modeling  apply first-order phrase model to transform sentences\n",
    "* Second-order phrase modeling  apply second-order phrase model to transform sentences\n",
    "* Apply text normalization and second-order phrase model to text of complete reviews\n",
    "* We'll use this transformed data as the input for some higher-level modeling approaches in the following sections.\n",
    "\n",
    "First, let's define a few helper functions that we'll use for text normalization. In particular, the **lemmatized_sentence_corpus** generator function will use spaCy to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuation or whitespace\n",
    "    \"\"\"\n",
    "\n",
    "    return token.is_punct or token.is_space\n",
    "\n",
    "def rules(token):\n",
    "    \"\"\"\n",
    "    conditions to select a specific token for the corpus cleaning\n",
    "    used with all() function : return True if all True\n",
    "    \"\"\"\n",
    "\n",
    "    return [not punct_space(token),\n",
    "            token not in nlp.Defaults.stop_words,\n",
    "            token.pos_ == 'NOUN' or token.pos_ ==\"ADJ\"]\n",
    "            \n",
    "def corpus_cleaning(serie):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse posts,\n",
    "    lemmatize the text, remove punctuations, unconvenient whitespaces, stopwords, and\n",
    "    keep only nouns and adjectives\n",
    "    \"\"\"\n",
    "    \n",
    "    for post in nlp.pipe(serie):\n",
    "      yield ' '.join([ token.lemma_ for token in post if all(rules(token)) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Update*\n",
    "\n",
    "At first, we have decided to keep verbs. However it appears that verbs like \"go\" or \"be\" appeared significantly in the topic modeling later. Therefore we will just keep Nouns & Adjectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "\"yield\" returns the value we ask him, but in opposite to \"return\", it saves the last value it returned. Therefore, in our case, it returns a \"list\" of posts, whereas \"return\" would return one string *''.join()* of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the cleaning is good !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original :\n",
      "write healthy future work employee skill productive rewarding career cost effective way apprenticeship worker master occupation professional identity pride article\n",
      "\n",
      "national disability advocate ms share able act\n",
      "\n",
      "month talent management hr company most modern apprenticeship program thank employer entrepreneuer insight program program safe happy festive season dimeny\n",
      "\n",
      "fortunate time assistant secretary policy evaluation research labor administration insight innovative thinking economic employment training policy many people young people full potential aged last project colleage resume thank institute report lerman lopr wonderful insight topic love thought conversation safe\n",
      "\n",
      "Cleaned :\n",
      "write healthy future work employee skill productive rewarding career cost effective way apprenticeship worker master occupation professional identity pride article\n",
      "\n",
      "national disability advocate ms share able act\n",
      "\n",
      "month talent management hr company most modern apprenticeship program thank employer entrepreneuer insight program program safe happy festive season dimeny\n",
      "\n",
      "fortunate time assistant secretary policy evaluation research labor administration insight innovative thinking economic employment training policy many people young people full potential aged last project colleage resume thank institute report lerman lopr wonderful insight topic love thought conversation safe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Original :\")\n",
    "\n",
    "for post in it.islice(corpus_cleaning(first_clean_corpus),4):\n",
    "    print(post)\n",
    "    print('')\n",
    "\n",
    "print(\"Cleaned :\")\n",
    "\n",
    "for post in it.islice(corpus_cleaning(first_clean_corpus),4):\n",
    "    print(post)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used *yield* instead of *return*.  \n",
    "The reason is well explained here : https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do\n",
    "\n",
    "Long story short, *yield* returns a generator, which is an iterable, as a list or a string for instance, but does not store all the values in the memory.\n",
    "\n",
    "Generators recquire less memory than a list that is very useful if the dataset is very large !\n",
    "\n",
    "Therefore, **we can only iterate over once**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how to use spacy with a pandas, here a clearly explained article : https://towardsdatascience.com/structured-natural-language-processing-with-pandas-and-spacy-7089e66d2b10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to apply the spaCy language model to the entire collection of posts. The easiest and most computationally efficient way to do this is to use the *nlp.pipe* function. This will iterate over each document and will apply the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'generator'>\n"
     ]
    }
   ],
   "source": [
    "#Let's apply the function *CorpusCleaning* to create a parsed list\n",
    "cleaned_posts = corpus_cleaning(first_clean_corpus)\n",
    "print(type(cleaned_posts))\n",
    "\n",
    "#We can print cleaned posts with \n",
    "#list(parsed_posts)\n",
    "#But it's time consuming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some examples.\n",
    "\n",
    "To iterate on a generator, we can use the *itertools* package, which is a common practice with generator objects.\n",
    "\n",
    "it.islice is an iterator designed to iterate over an object. Because we can't iterate directly over a generator (can't be subscriptable), this function is pretty useful !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write healthy future work employee skill productive rewarding career cost effective way apprenticeship worker master occupation professional identity pride article\n",
      "<class 'str'>\n",
      "----\n",
      "national disability advocate ms share able act\n",
      "<class 'str'>\n",
      "----\n",
      "month talent management hr company most modern apprenticeship program thank employer entrepreneuer insight program program safe happy festive season dimeny\n",
      "<class 'str'>\n",
      "----\n",
      "fortunate time assistant secretary policy evaluation research labor administration insight innovative thinking economic employment training policy many people young people full potential aged last project colleage resume thank institute report lerman lopr wonderful insight topic love thought conversation safe\n",
      "<class 'str'>\n",
      "----\n",
      "online job platform different way time workplace school international border example future work conversation\n",
      "<class 'str'>\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for post in it.islice(cleaned_posts,5):\n",
    "    print(post)\n",
    "    print(type(post))\n",
    "    print('----')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**\n",
    "\n",
    "When we iterate over a generator, the value saved in it is then deleted ! \n",
    "\n",
    "That confirms the definition of a generator : we can only iterate over it once !\n",
    "\n",
    "**That also means if we want to use several times a generator, we have to recreate one for the purpose !**\n",
    "\n",
    "For instance, here, we can't use anymore the 6 first values contained in the generator !\n",
    "\n",
    "A common practice consists in iterating on a generator in this way :\n",
    "\n",
    "```\n",
    "for i in create_a_generator_function(y):   \n",
    "    print(i)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's group words together with the **gensim.models.Phrases** library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was long to run\n",
    "# Make the folloing statement true if you want to run it again\n",
    "if 1 == 1:\n",
    "\n",
    "    # Let's generate the cleaned content one more time as a generator\n",
    "    # It can be used only one time, so we have to be careful to save the modified version\n",
    "    cleaned_posts = corpus_cleaning(first_clean_corpus)\n",
    "\n",
    "    # gensim.Phrases needs a sequence of sentences. (e.g. an iterable or a generator)\n",
    "    # Each sentence has to be a list of string tokens\n",
    "    streamed_posts = (post.split(' ') for post in cleaned_posts)\n",
    "\n",
    "    # We train the model\n",
    "    # min_count : the minimum of sentences (aka. coupled words) found in the doc. Under this value, the model doesn't take it into consideration.\n",
    "    # treshold : (by default : 10). The higher, the fewer phrases.\n",
    "    # This line return a model we can use later. \n",
    "    bigram_model = Phrases(streamed_posts,min_count=5,threshold=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrases</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cost_effective</td>\n",
       "      <td>31.947798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apprenticeship_program</td>\n",
       "      <td>57.727183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>festive_season</td>\n",
       "      <td>149.195370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>young_people</td>\n",
       "      <td>11.238823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>full_potential</td>\n",
       "      <td>29.053028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>infinite_mindset</td>\n",
       "      <td>113.322901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>bit_optimism</td>\n",
       "      <td>233.523188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>optimism_podcast</td>\n",
       "      <td>42.723320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>worthy_rival</td>\n",
       "      <td>434.901484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>chapter_infinite</td>\n",
       "      <td>341.922546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2198 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Phrases       Score\n",
       "0             cost_effective   31.947798\n",
       "1     apprenticeship_program   57.727183\n",
       "2             festive_season  149.195370\n",
       "3               young_people   11.238823\n",
       "4             full_potential   29.053028\n",
       "...                      ...         ...\n",
       "2193        infinite_mindset  113.322901\n",
       "2194            bit_optimism  233.523188\n",
       "2195        optimism_podcast   42.723320\n",
       "2196            worthy_rival  434.901484\n",
       "2197        chapter_infinite  341.922546\n",
       "\n",
       "[2198 rows x 2 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many sentences were found ? / Which words were associated in the doc ?\n",
    "# pd.DataFrame(data = bigram_model.export_phrases(), columns=[['Phrases',\"Score\"]])\n",
    "\n",
    "# model.export_phrases() return a dict with phrases as keys and scores as values\n",
    "# score represents the frequency of apparition in doc, following the previous formula :\n",
    "phrases = bigram_model.export_phrases()\n",
    "\n",
    "pd.DataFrame(data = {\"Phrases\" : phrases.keys() , \"Score\" : phrases.values()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ score(A,B) = \\frac{count(AB)-min_{AB}}{count(A)*count(B)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes**\n",
    "\n",
    "We see that some phrases have a huge score. \n",
    "\n",
    "Let's check these posts.\n",
    "\n",
    "*Update : by removing hashtags at the begining, we avoid this issue*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text ='startwithwhy'\n",
    "# serie = corpus.Content\n",
    "\n",
    "# for post in serie.loc[serie.str.contains(text, regex=False)]:\n",
    "#     print(post)\n",
    "#     print(\"....\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, some words are associated together because they are written together in the hashtags \"section\".  \n",
    "\n",
    "It would be maybe useful to remove hashtags from all posts to avoid this issue.\n",
    "\n",
    "*Update : that is what we have done*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we have defined *bigram phrases*, we can apply the model to the corpus !**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was long to run\n",
    "# Make the folloing statement true if you want to run it again\n",
    "if 1 == 1:\n",
    "\n",
    "    # We re-create from the first_clean_corpus a sequence of posts, each of them tokenized (words are splitted)\n",
    "    cleaned_posts = corpus_cleaning(first_clean_corpus)\n",
    "    streamed_posts = (post.split(' ') for post in cleaned_posts)\n",
    "\n",
    "    # We create a new list of cleaned posts we will use for future models\n",
    "    bigram_corpus = []\n",
    "\n",
    "    for streamed_post in streamed_posts:\n",
    "        bigram_post = ' '.join(bigram_model[streamed_post])\n",
    "        bigram_corpus.append(bigram_post)\n",
    "\n",
    "    bigram_corpus   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**J'AI REUSSI !!!!!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling with Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Now we have succesfully cleaned the corpus, removed hashtags, selected only relevant words, and associated words together, we can now apply a model to define topics in all these posts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is family of techniques that can be used to describe and summarize the documents in a corpus according to a set of latent \"topics\". For this demo, we'll be using Latent Dirichlet Allocation or LDA, a popular approach to topic modeling.\n",
    "\n",
    "In many conventional NLP applications, documents are represented a mixture of the individual tokens (words and phrases) they contain. In other words, a document is represented as a vector of token counts. There are two layers in this model — documents and tokens — and the size or dimensionality of the document vectors is the number of tokens in the corpus vocabulary. This approach has a number of disadvantages:\n",
    "\n",
    "* Document vectors tend to be large (one dimension for each token  lots of dimensions)\n",
    "* They also tend to be very sparse. Any given document only contains a small fraction of all tokens in the vocabulary, so most values in the document's token vector are 0.\n",
    "* The dimensions are fully indepedent from each other — there's no sense of connection between related tokens, such as knife and fork.\n",
    "\n",
    "LDA injects a third layer into this conceptual model. Documents are represented as a mixture of a pre-defined number of topics, and the topics are represented as a mixture of the individual tokens in the vocabulary. The number of topics is a model hyperparameter selected by the practitioner. LDA makes a prior assumption that the (document, topic) and (topic, token) mixtures follow Dirichlet probability distributions. This assumption encourages documents to consist mostly of a handful of topics, and topics to consist mostly of a modest set of the tokens.\n",
    "\n",
    "LDA is fully unsupervised. The topics are \"discovered\" automatically from the data by trying to maximize the likelihood of observing the documents in your corpus, given the modeling assumptions. They are expected to capture some latent structure and organization within the documents, and often have a meaningful human interpretation for people familiar with the subject material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll again turn to gensim to assist with data preparation and modeling. In particular, gensim offers a high-performance parallelized implementation of LDA with its *LdaModel* class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to creating an LDA model is to learn the full vocabulary of the corpus to be modeled. We'll use gensim's **Dictionary** class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's create a new corpus as a save\n",
    "corpus = pd.Series(bigram_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary recquired to split the sentences into a list of words. Let's use split() function\n",
    "streamed_corpus = corpus.apply(lambda str : str.split())\n",
    "\n",
    "# Then we learn the dictionary by iterating over all of the reviews\n",
    "# It return a generator\n",
    "corpus_dictionary = Dictionary(streamed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we take care to remove from the dictionary words that\n",
    " appears rarely, and too often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary (filter_extremes) and reassign integer ids (compactify)\n",
    "# We remove words that appears less than 10 times, and more than 40% of the time.\n",
    "corpus_dictionary.filter_extremes(no_below=10, no_above=0.4)\n",
    "corpus_dictionary.compactify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dictionary** encapsulates the mapping between normalized words and their integer ids (https://radimrehurek.com/gensim/corpora/dictionary.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(536, 1), (630, 1), (1430, 1), (1618, 1), (2559, 1)]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see some words as an example\n",
    "corpus_dictionary.doc2bow([\"entrepreneur\",\"host\",\"startup\",\"guy\",'event'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "It seems that Dictionary() returns a generator. However, calling this generator with doc2bow doesn't seem to erase it. Maybe data present in a generator are deleted when we iterate over it only. We have to check that.\n",
    "\n",
    "Anyway, let's resume the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like many NLP techniques, LDA uses a simplifying assumption known as the bag-of-words model. In the bag-of-words model, a document is represented by the counts of distinct terms that occur within it. Additional information, such as word order, is discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = [corpus_dictionary.doc2bow(post) for post in streamed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was long to run\n",
    "# Make the folloing statement true if you want to run it again\n",
    "if 1 == 1: \n",
    "\n",
    "    # Train the model on the corpus.\n",
    "    lda = LdaModel(corpus=bag_of_words,id2word=corpus_dictionary, num_topics=10,passes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our topic model is now trained and ready to use! Since each topic is represented as a mixture of tokens, you can manually inspect which tokens have been grouped together into which topics to try to understand the patterns the model has discovered in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "        \n",
    "    print ('{:20}{}'.format('term', 'frequency'))\n",
    "\n",
    "    for term, frequency in lda.show_topic(topic_number, topn=25):\n",
    "        print (\"{:20}{:.03f}\".format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Running a second time Lda changes topics' ID !\n",
    "We have done that and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                frequency\n",
      "company             0.049\n",
      "business            0.035\n",
      "brand               0.025\n",
      "team                0.017\n",
      "marketing           0.017\n",
      "product             0.015\n",
      "new                 0.015\n",
      "customer            0.013\n",
      "more                0.013\n",
      "strategy            0.011\n",
      "great               0.010\n",
      "consumer            0.010\n",
      "startup             0.009\n",
      "employee            0.008\n",
      "industry            0.008\n",
      "sale                0.008\n",
      "client              0.008\n",
      "big                 0.007\n",
      "opportunity         0.007\n",
      "ceo                 0.007\n",
      "global              0.006\n",
      "growth              0.006\n",
      "founder             0.006\n",
      "investment          0.006\n",
      "digital             0.005\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the power of topic modeling, a human view is necessary to assess each topic. We have selected 10 topics after several trials, and we will name each of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {0: 'People',\n",
    "               1: 'Tech',\n",
    "               2: 'World & Politics',\n",
    "               3: 'Career',\n",
    "               4: 'Content',\n",
    "               5: 'Interview',\n",
    "               6: 'Personal devlopment',\n",
    "               7: 'Health',\n",
    "               8: 'Money', \n",
    "               9: 'Business'} #Business, strategy, team, community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually reviewing the top terms for each topic is a helpful exercise, but to get a deeper understanding of the topics and how they relate to each other, we need to visualize the data — preferably in an interactive format. Fortunately, we have the fantastic pyLDAvis library to help with that!\n",
    "\n",
    "pyLDAvis includes a one-line function to take topic models created with gensim and prepare their data for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeremy\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "LDAvis_prepared = gensimvis.prepare(lda, bag_of_words,\n",
    "                                              corpus_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyLDAvis.display(...) displays the topic model visualization in-line in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el1406425577926794089102605968\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el1406425577926794089102605968_data = {\"mdsDat\": {\"x\": [0.20027644585522142, 0.07538165914160111, 0.18777418222360678, -0.039113903119744615, -0.005231230631475678, -0.08684875964795817, -0.10985771471767951, 0.20881985923409638, -0.16761792140106424, -0.26358261693660323], \"y\": [-0.01801423227248615, 0.17520168807022682, 0.02086748625478806, 0.171643571397714, -0.19904821243831367, -0.13437017622949776, 0.22580757400133225, -0.08778298910487754, -0.17766478098776842, 0.023360071308882038], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [16.52213849598274, 14.490523850474682, 12.037751508046892, 11.968167876065907, 9.397729554438731, 8.493495730671942, 7.355793961150598, 7.188166716598089, 6.655969621407511, 5.8902626851629085]}, \"tinfo\": {\"Term\": [\"people\", \"company\", \"time\", \"video\", \"business\", \"technology\", \"day\", \"thing\", \"new\", \"content\", \"question\", \"brand\", \"life\", \"health\", \"book\", \"good\", \"future\", \"woman\", \"tech\", \"work\", \"story\", \"world\", \"datum\", \"team\", \"market\", \"way\", \"today\", \"podcast\", \"digital\", \"marketing\", \"ad\", \"mistake\", \"empathy\", \"journalist\", \"emotion\", \"language\", \"influence\", \"belief\", \"freedom\", \"possibility\", \"curiosity\", \"understanding\", \"robot\", \"race\", \"humanity\", \"african\", \"planet\", \"ideal\", \"thankful\", \"speech\", \"trade\", \"rare\", \"kindness\", \"confident\", \"equal\", \"none\", \"reward\", \"wealth\", \"pr\", \"journalism\", \"people\", \"passion\", \"person\", \"behavior\", \"thing\", \"choice\", \"different\", \"decision\", \"problem\", \"wrong\", \"relationship\", \"ability\", \"situation\", \"other\", \"word\", \"easy\", \"power\", \"bad\", \"way\", \"right\", \"job\", \"well\", \"value\", \"same\", \"reason\", \"life\", \"true\", \"hard\", \"many\", \"lot\", \"time\", \"good\", \"important\", \"most\", \"own\", \"human\", \"leader\", \"more\", \"world\", \"much\", \"idea\", \"new\", \"woman\", \"\\ud83d\\udda4\", \"journey\", \"school\", \"incredible\", \"student\", \"lesson\", \"young\", \"personal_brand\", \"man\", \"class\", \"\\ud83c\\udf57\", \"college\", \"tv\", \"link_comment\", \"speaker\", \"mentor\", \"year_old\", \"expectation\", \"mother\", \"personal_branding\", \"grateful\", \"clarity\", \"fellow\", \"inspiring\", \"chicken\", \"confidence\", \"female\", \"honest\", \"ticket\", \"community\", \"talk\", \"child\", \"black\", \"story\", \"career\", \"excited\", \"amazing\", \"event\", \"first\", \"thank\", \"proud\", \"year\", \"friend\", \"opportunity\", \"advice\", \"today\", \"entrepreneur\", \"business\", \"great\", \"team\", \"day\", \"many\", \"world\", \"part\", \"new\", \"more\", \"book\", \"fun\", \"habit\", \"newsletter\", \"productivity\", \"productive\", \"art\", \"holiday\", \"author\", \"secret\", \"inspiration\", \"happiness\", \"joy\", \"magic\", \"movie\", \"netflix\", \"busy\", \"schedule\", \"host\", \"play\", \"mental\", \"fan\", \"reading\", \"late_episode\", \"disruption\", \"tune\", \"storytelling\", \"birthday\", \"spirit\", \"funny\", \"happy\", \"game\", \"day\", \"meeting\", \"minute\", \"time\", \"work\", \"list\", \"tip\", \"hour\", \"little\", \"life\", \"good\", \"week\", \"ready\", \"idea\", \"year\", \"thing\", \"goal\", \"home\", \"new\", \"way\", \"more\", \"next\", \"friend\", \"great\", \"today\", \"podcast\", \"company\", \"brand\", \"marketing\", \"customer\", \"sale\", \"ceo\", \"capital\", \"advertising\", \"retail\", \"revenue\", \"venture\", \"presentation\", \"\\ud83d\\udc47\", \"transunion\", \"ecosystem\", \"small_business\", \"mobile\", \"contact\", \"cloud\", \"funding\", \"sustainable\", \"diverse\", \"buyer\", \"edge\", \"big_tech\", \"bright\", \"product_service\", \"\\u2714\", \"offering\", \"marketplace\", \"consumer\", \"strategy\", \"product\", \"business\", \"employee\", \"startup\", \"investment\", \"team\", \"client\", \"building\", \"founder\", \"industry\", \"amazon\", \"campaign\", \"growth\", \"role\", \"new\", \"great\", \"opportunity\", \"global\", \"more\", \"big\", \"top\", \"insight\", \"experience\", \"digital\", \"value\", \"year\", \"late_article\", \"worker\", \"creator\", \"million\", \"anxiety\", \"lockdown\", \"war\", \"pitch\", \"annual\", \"framework\", \"emotional_intelligence\", \"indian\", \"economist\", \"virgin\", \"take\", \"caregiver\", \"investing\", \"profit\", \"insurance\", \"weekly\", \"thousand\", \"motivation\", \"search\", \"authority\", \"cold\", \"inclusion\", \"soft_skill\", \"peace\", \"election\", \"travel\", \"money\", \"fear\", \"rule\", \"price\", \"return\", \"pressure\", \"bill\", \"finance\", \"cost\", \"financial\", \"uncertainty\", \"hand\", \"risk\", \"eye\", \"name\", \"pandemic\", \"change\", \"medium\", \"effort\", \"more\", \"other\", \"good\", \"many\", \"view\", \"time\", \"high\", \"economy\", \"year\", \"support\", \"number\", \"way\", \"most\", \"health\", \"patient\", \"care\", \"feedback\", \"medical\", \"study\", \"hospital\", \"marketer\", \"review\", \"medicine\", \"body\", \"disease\", \"percent\", \"algorithm\", \"treatment\", \"drug\", \"vaccine\", \"identity\", \"doctor\", \"\\u2705\", \"mask\", \"iphone\", \"cancer\", \"physician\", \"method\", \"favourite\", \"accounting\", \"battle\", \"addiction\", \"meditation\", \"healthcare\", \"artificial_intelligence\", \"research\", \"brain\", \"performance\", \"machine\", \"issue\", \"such\", \"digital_health\", \"datum\", \"new\", \"science\", \"result\", \"stress\", \"technology\", \"tech\", \"innovation\", \"app\", \"phone\", \"payment\", \"apple\", \"task\", \"internet\", \"device\", \"smartphone\", \"player\", \"zoom\", \"uber\", \"impossible\", \"blockchain\", \"computer\", \"camera\", \"cliq\", \"focused\", \"hack\", \"radical\", \"retailer\", \"transformation\", \"budget\", \"medical_futurist\", \"google\", \"disability\", \"g\", \"title\", \"digital\", \"future\", \"change\", \"solution\", \"firm\", \"digital_health\", \"new\", \"smart\", \"trend\", \"service\", \"world\", \"exciting\", \"access\", \"next\", \"need\", \"today\", \"key\", \"big\", \"first\", \"year\", \"video\", \"content\", \"social_medium\", \"email\", \"distraction\", \"channel\", \"profile\", \"indistractable\", \"account\", \"relevant\", \"valuable\", \"guide\", \"read\", \"pro\", \"candidate\", \"sleep\", \"technique\", \"okay\", \"authentic\", \"automation\", \"popular\", \"financial_time\", \"regulation\", \"bot\", \"livestream\", \"video_game\", \"insightful\", \"privacy\", \"recommendation\", \"lucky\", \"skill\", \"question\", \"linkedin\", \"comment\", \"audience\", \"useful\", \"post\", \"topic\", \"article\", \"series\", \"link\", \"writing\", \"connection\", \"message\", \"tool\", \"voice\", \"free\", \"platform\", \"time\", \"attention\", \"thought\", \"good\", \"sure\", \"way\", \"more\", \"country\", \"failure\", \"banking\", \"test\", \"data\", \"source\", \"chart\", \"influencer\", \"alternative\", \"population\", \"theme\", \"road\", \"intention\", \"intentional\", \"driver\", \"video_credit\", \"genius\", \"uncomfortable\", \"tropical_disease\", \"testing\", \"forum\", \"psychology\", \"claim\", \"tree\", \"legacy\", \"access_control\", \"segment\", \"citizen\", \"masterclass\", \"fall\", \"news\", \"priority\", \"datum\", \"report\", \"number\", \"morning\", \"global\", \"state\", \"system\", \"world\", \"policy\", \"bank\", \"case\", \"crisis\", \"week\", \"big\", \"program\", \"economic\", \"government\", \"food\", \"late\", \"more\", \"impact\", \"today\", \"episode\", \"show\", \"discussion\", \"\\u202c\", \"award\", \"pm\", \"magazine\", \"guest\", \"wisdom\", \"think\", \"asset\", \"calendar\", \"financial_service\", \"facebook\", \"bitcoin\", \"\\u2764\", \"remote\", \"chinese\", \"et\", \"past_year\", \"new_episode\", \"entertainment\", \"tactic\", \"hot\", \"crypto\", \"production\", \"currency\", \"air\", \"round\", \"window\", \"market\", \"tomorrow\", \"podcast\", \"conversation\", \"bank\", \"economy\", \"growth\", \"today\", \"weekend\", \"high\", \"interesting\", \"stock\", \"future\", \"week\", \"investor\", \"big\", \"link\"], \"Freq\": [3940.0, 2384.0, 4953.0, 1419.0, 2289.0, 1319.0, 2403.0, 2578.0, 3700.0, 1009.0, 1262.0, 1233.0, 2391.0, 921.0, 1060.0, 3106.0, 1267.0, 1111.0, 807.0, 1625.0, 1378.0, 2415.0, 1033.0, 1268.0, 886.0, 2603.0, 2028.0, 818.0, 884.0, 813.0, 378.26481015451407, 225.94201325124328, 203.3361556870697, 173.48585205453836, 157.4081584358627, 145.3259679986733, 139.61785475334673, 133.0932593623171, 128.5059527177644, 124.87159815671406, 115.77264133897783, 109.11233958730587, 107.0814580978746, 105.04477965533326, 91.55767737375163, 89.93189694090394, 87.20504603930895, 83.13061391292362, 82.57089627365723, 78.93657287902147, 78.78697408908445, 77.7258433806089, 77.61291187741374, 77.05945089587232, 76.02296502971815, 74.58784513800371, 74.4670285319223, 74.33184420872402, 74.13429588997074, 70.43906552966115, 3695.122773846501, 177.7393819822514, 588.3548992942283, 235.4276673577594, 1895.1007657237735, 264.07156583189976, 525.6790525950497, 317.3439754261985, 540.4721289379704, 264.38064316519257, 339.684466553514, 291.7025558325084, 206.77325021870155, 1618.9442668102022, 478.75424432343635, 485.9999370483769, 436.27236428087923, 482.75351864108563, 1408.9084383982547, 529.4327978987149, 667.1781900478907, 687.9623862636428, 547.1267817770015, 502.50888034546057, 322.5173509135367, 1038.846098729572, 290.803248940085, 356.14619273252384, 853.5770979856457, 534.4008490468646, 1399.4948098758105, 1051.7398314357551, 532.2004378453786, 448.6376080951953, 445.4821021192053, 377.0577660846244, 471.9976149759315, 801.3750365669316, 670.0110924666644, 410.8609216603027, 410.9204183457172, 421.78490579923397, 1110.8155628520076, 518.3037726175029, 422.2342479268614, 333.25119623706064, 329.0021114477009, 318.4026334535292, 317.3911353343252, 303.6697485524588, 291.39512799005865, 251.67437177046094, 249.5332375812031, 241.87149125468304, 216.56746014705226, 200.82093140046098, 199.6253244964469, 198.99561532414148, 175.10121707035233, 159.24406724854367, 153.46184050517684, 153.14035056789763, 152.68044078260735, 151.84366218719614, 147.92531992566688, 145.7474897162758, 144.82645377854746, 141.01580784426218, 134.93024100150697, 134.47560669620606, 131.737591580587, 127.30662573501365, 738.6298482524168, 268.2831408202877, 350.1660251285501, 213.57623565899038, 1033.754631258582, 622.2070570832294, 410.10969153617935, 458.2058599218944, 546.8615038432362, 892.6996126576114, 515.198838301691, 282.78754003622294, 1047.6999389320176, 432.01722922278304, 420.490496784678, 260.98053714444245, 570.8616030354651, 305.50826699579835, 582.7148685510465, 525.7665814136468, 432.59958207860365, 494.6241383340579, 432.5563942113332, 418.13377860227973, 337.4700221852379, 405.6831264796127, 321.25929220072254, 1059.517294584973, 348.27896533480487, 320.59555820647336, 267.2969951526827, 259.23737106595405, 198.3895310350442, 187.56809459116707, 184.94979252246048, 170.87093366517968, 163.70357117244407, 158.63020998813352, 154.3570776131554, 151.3200070730176, 139.32909802228372, 137.62309676110652, 123.65726360963578, 119.87256577649077, 119.64341481512821, 117.02656601516533, 109.74960662256187, 102.53263998028491, 95.97775078739093, 87.52577252401429, 87.1918329386664, 84.41068529502925, 83.02122413209258, 82.1550191925279, 78.47954926523379, 74.7352193775441, 74.59709072491877, 495.18806334374284, 386.11974456676995, 1762.9971896955474, 327.3268038321496, 319.251738765019, 2606.8055321629513, 1020.8120248164142, 393.2313151641414, 454.9814307189814, 336.43066963321013, 360.2836236702937, 1081.5617464997993, 1216.7303631820669, 513.335348049055, 255.2402916303095, 454.8860145430269, 769.6450411088219, 682.70832495665, 327.9946716491833, 312.89559542649016, 740.7021495408198, 583.7267357979716, 588.3306003487656, 305.9573725414551, 288.75793695958305, 332.2823367115986, 320.76436445398554, 275.58214838257663, 2383.6714853831363, 1232.5457168253058, 812.4877653849355, 626.9077204580155, 381.3182808512313, 318.2757240368544, 214.3850952957854, 193.10503136703935, 185.9874300297259, 168.1700604586006, 149.7002538811833, 131.47667316266055, 131.08180648820078, 126.94856911971893, 125.32286663963842, 122.69798136252079, 112.07606669074971, 107.85270566860196, 105.55632472126702, 94.08753103465911, 92.53880729407943, 92.08327873149334, 84.49924459193434, 78.64428873659817, 74.83812211773673, 74.77385934715556, 70.61439423452383, 68.0705457446892, 66.5625718024229, 66.33593632440723, 472.63426684295933, 529.9170146302503, 729.1334546692135, 1706.0225224345006, 411.9156676399838, 431.06229337448445, 285.96332385271523, 834.6593581421552, 371.30929507104395, 174.60473581310004, 291.3760032959417, 387.03363760525406, 139.48082530599427, 192.81492892475308, 299.8865027254091, 251.44720618228112, 728.1965027968582, 492.76457690549876, 337.96613946219935, 307.666350369495, 626.7537516909812, 352.9615685680751, 247.5041383019392, 223.85096866709483, 256.83586469176504, 257.0560264829849, 239.34417744222893, 227.19640294026146, 261.4535783004015, 235.3721789335052, 186.55568117683364, 150.56721709205073, 145.52405531165573, 126.68656925035349, 125.66496048278267, 117.51235268966853, 115.68355617255867, 115.44023959341808, 114.44570047113142, 105.47812262705077, 103.92957759802029, 103.27942747307175, 102.54119020508655, 94.41439341625122, 93.30259681607846, 92.3576056037913, 90.82919883084648, 90.36234938188888, 89.97888046770144, 88.74439296051386, 86.55891353709106, 85.57610104895991, 82.54846670158877, 82.53694393182144, 81.1361474022152, 80.81926236994478, 80.20150895538052, 79.61129496056014, 645.1355573645103, 303.55641402516403, 210.85823080172688, 181.82641737223864, 106.181277492929, 108.63707223142413, 125.09375019440934, 148.9451833829119, 169.92685303352465, 205.2664256343644, 119.51313984105926, 164.4811388594737, 223.00938152280676, 134.58002995140055, 152.58754863370064, 210.0537287439105, 241.10284234023297, 199.9013176666704, 162.83405642159454, 384.367832047998, 335.7555709536177, 316.21547071024935, 259.20272045405267, 156.61876304579127, 303.1553152732326, 180.6062801452459, 156.27366495492902, 209.18322780633457, 145.49282860558628, 153.6237115458545, 179.7063340990799, 145.57372731302195, 920.9113021133511, 482.2050958865671, 429.5264775402792, 389.25011712188694, 337.97618032793025, 335.2040835655211, 254.13309461213632, 220.00002502547576, 194.3806323711681, 193.82794976481708, 188.69471138995536, 182.05548661234616, 178.8506132218423, 175.1114821147516, 156.57711646212886, 149.53839200950335, 148.56661525841295, 139.88548011056898, 139.57922639940247, 136.85239077923998, 133.8444874711364, 133.20457502798465, 117.34028031340759, 114.9672826849329, 111.13558653383147, 107.99374228796425, 106.86910303840459, 105.60689394887991, 104.90426478313536, 103.10686071089232, 343.9457484816511, 175.6527733327354, 236.05561816073438, 194.61214511646529, 158.84063820211804, 136.30645328470695, 222.03801009310862, 182.51732650133835, 182.50937976627486, 228.75585307425922, 315.80408454849265, 152.42272038304102, 161.88047264526503, 144.89684184505174, 1318.343556930001, 806.2746760611003, 496.5728354824573, 457.08491323790884, 342.6882360998815, 314.60529269938957, 313.4199601446144, 312.8437086403907, 266.1238508311565, 262.92056959441015, 166.39136610751083, 155.93400393163282, 142.97592113575195, 135.0945128005823, 130.67578907012103, 130.40099771953962, 124.88482915469207, 112.4903572388994, 112.35700628582111, 111.34600611484235, 108.40447321365284, 105.57950185258446, 101.58452347939475, 96.710744217829, 87.75054672242258, 86.07911903589402, 83.07666554165624, 80.63941167774104, 80.41240077634802, 79.81595341215808, 626.8673383718761, 759.0330421486918, 391.36931935127785, 318.73007682316955, 243.11969097465635, 277.78669285735833, 1088.2376868271335, 221.81102594251468, 200.08186243130967, 259.52272542503135, 355.051432530711, 143.09175420960844, 166.84091998387356, 210.38275005353657, 182.74318591059256, 256.1431969788785, 184.06089119231922, 219.59340812628443, 181.45009585083778, 187.16869478397618, 1418.4114557395008, 1008.9660834258442, 586.3849053503076, 504.4862959558367, 440.2657917020371, 253.76648620490053, 230.13355042969553, 197.43598881279604, 181.71716841554456, 169.47550657246782, 159.97719862503038, 150.66819269980024, 112.74820849431838, 106.88734412394174, 104.20422084091219, 101.70770805001268, 98.8100352434973, 98.50621776906227, 97.23490031679216, 95.77467406333362, 92.75193663380922, 91.1498253892772, 90.44739027650327, 89.0088860944992, 86.11804719810432, 85.87686712010127, 83.55712751618809, 81.50477820508543, 77.07706207524937, 76.9686551081591, 373.43503208780254, 895.841820044061, 424.5469306267492, 430.0303570564223, 283.7603821303725, 133.64880056241157, 344.5640397986799, 244.17547301243545, 391.23321879202, 216.75783916828718, 305.0478765623123, 134.88416541014888, 182.2405049135828, 246.996603313732, 211.69605414557324, 211.79896957774525, 267.80217148872094, 216.00567515597558, 533.1294370035603, 201.85564391174591, 225.42235584364025, 339.66404488620003, 214.58100828603102, 269.9863650955712, 207.39760603584412, 543.2122405916087, 265.71817165816185, 228.58201945250826, 222.6674281724663, 182.4786594308932, 138.60854797017964, 138.15042509747877, 137.33042935049093, 123.92565214509614, 103.86001973052767, 101.12427109637828, 99.04989906560058, 98.95362395630794, 97.00286022638915, 93.28552734021847, 93.2796137282627, 91.26332943785309, 90.35779348225662, 88.55659642414909, 88.32497995588147, 88.21304821366134, 78.47806900088192, 76.56858755569894, 76.55108527318018, 76.15615392696209, 74.96997251225454, 74.03964584156962, 73.67431394372888, 73.0023279940671, 71.58621881769076, 537.1991393340616, 120.41162321135388, 636.5546001282556, 220.91446283480056, 289.02189583713516, 226.5289037806289, 351.1925395425747, 194.04643774274126, 267.12588752330157, 603.3540508449619, 178.36741846681528, 228.47404267369805, 181.47144952706358, 163.86908760086087, 293.5262601641581, 308.2001035174367, 153.8090041180672, 156.72162750471531, 140.8607050228123, 135.30717051349654, 162.32022348602524, 241.17675737435667, 147.04617990783456, 162.5789283147909, 390.5211392232455, 344.3203777662556, 251.0583037399037, 235.09263061804847, 189.84299758354135, 181.23700868450808, 163.61637744083868, 163.4880003258955, 153.34544158267656, 152.05640379377078, 139.39858025954774, 139.35670269799152, 137.41633493868858, 124.66073413152978, 123.00623156291906, 120.72214335608564, 117.82603608699675, 111.47298000010234, 109.97637661622916, 109.5628231959258, 108.28878532957268, 97.50319657310234, 96.67488973869143, 96.31862491148546, 89.59664295164526, 83.33681416117899, 83.07650994962185, 82.90656638262723, 80.62761148262295, 79.78153251775058, 597.593403404477, 285.59672541169414, 542.5186210990187, 462.35623499212954, 332.23950680867614, 272.8297246633953, 278.02376451228105, 534.1528865150639, 138.77398348445553, 225.13134871664167, 191.97003104572167, 128.53421413204168, 211.8002619919334, 153.41983008748394, 132.55126005796302, 146.77969770503935, 131.7396864704886], \"Total\": [3940.0, 2384.0, 4953.0, 1419.0, 2289.0, 1319.0, 2403.0, 2578.0, 3700.0, 1009.0, 1262.0, 1233.0, 2391.0, 921.0, 1060.0, 3106.0, 1267.0, 1111.0, 807.0, 1625.0, 1378.0, 2415.0, 1033.0, 1268.0, 886.0, 2603.0, 2028.0, 818.0, 884.0, 813.0, 379.1786734236319, 226.85581222713245, 204.24989784906234, 174.39974966376298, 158.32193492003995, 146.23977860423315, 140.53167007917045, 134.00705254751006, 129.41972305495764, 125.78542201668805, 116.6864628973952, 110.02615315993387, 107.99528506933997, 105.95866687938309, 92.4714746711699, 90.8459649164204, 88.11882191130088, 84.04451551701801, 83.48485479680053, 79.85037024883721, 79.70089521893074, 78.63970486525395, 78.5267194088655, 77.97327690064206, 76.93676241406057, 75.5016511235569, 75.38091969957674, 75.24569760338112, 75.04807081923924, 71.35299533742035, 3940.784198409573, 187.28594665795194, 677.6906866199885, 263.11799120478906, 2578.6280945702624, 298.3900476485782, 652.5295340324677, 372.96860404043167, 695.7430154460687, 313.79513673893365, 419.69271407291586, 358.05658561546454, 241.41867803934616, 2771.9344570354247, 669.4770570333471, 689.1924877561215, 609.5676389377187, 691.6770010740388, 2603.293255307125, 799.1820445818689, 1086.9505679164283, 1155.345495016484, 903.4193233882941, 806.0979635881845, 449.3315033084838, 2391.1984245489916, 393.6163336627984, 532.8293788924875, 2057.382607155039, 1001.8278568303798, 4953.040069355677, 3106.1898081135114, 1045.4222735077435, 797.9571962761618, 801.4400103478646, 592.6041479648327, 915.4640780693657, 3529.072844120012, 2415.4478319045343, 743.571167021124, 1060.7318078409544, 3700.8154783403234, 1111.7310138720204, 519.2196798905361, 423.14978195957383, 334.1666692609892, 329.9176597215042, 319.3180639354942, 318.30668740679283, 304.58522911068417, 292.31080964687203, 252.58984630122544, 250.44871705508942, 242.78734921709093, 217.48291849586357, 201.7365514601009, 200.54114905649178, 199.91113586793992, 176.01667988844713, 160.15956299436075, 154.377492699922, 154.0558107916551, 153.59618540182234, 152.75915994130384, 148.84110649268902, 146.66297690805408, 145.74191754548832, 141.93148643807206, 135.8458577957157, 135.39105034182074, 132.65320842583148, 128.22215358945263, 810.4736248430889, 290.29300309519977, 398.9237947047905, 228.63861610175692, 1378.477282276447, 799.7760717293597, 493.24892766084724, 574.4385600916887, 740.397976896079, 1464.5823402625094, 734.504785775247, 361.981311246616, 2632.1284931746563, 721.5958731208553, 920.5631995150203, 353.22420141450056, 2028.0177088248483, 515.6146974801875, 2289.5597729721003, 1945.404767613406, 1268.0813298318756, 2403.0807764489837, 2057.382607155039, 2415.4478319045343, 962.8273444248589, 3700.8154783403234, 3529.072844120012, 1060.4287733783751, 349.19048678290943, 321.5070835407911, 268.2085073426492, 260.148885188716, 199.301039945697, 188.47961741788149, 185.86126629136848, 171.78246085525805, 164.6151853795583, 159.5417573488621, 155.26855080961852, 152.23146974723326, 140.2407850730887, 138.53462306386317, 124.56896393868796, 120.78411152566153, 120.55491815985997, 117.93817333211324, 110.66112739718065, 103.44420597054123, 96.88932883306305, 88.43724892196794, 88.10352957112757, 85.32232461227859, 83.9329111315213, 83.06677132692788, 79.3910040872688, 75.64677731503804, 75.50859938229414, 538.3235010338157, 428.9821955238559, 2403.0807764489837, 387.7098881163585, 389.7441255338791, 4953.040069355677, 1625.8566500036932, 523.8126120215196, 638.8534811058242, 455.0216966529033, 498.18266404851136, 2391.1984245489916, 3106.1898081135114, 1162.597910495928, 390.9038644962403, 1060.7318078409544, 2632.1284931746563, 2578.6280945702624, 666.5380969081315, 616.5509718297365, 3700.8154783403234, 2603.293255307125, 3529.072844120012, 815.9009389943592, 721.5958731208553, 1945.404767613406, 2028.0177088248483, 818.9107462552392, 2384.584661336967, 1233.458896498527, 813.4009352711635, 627.820857470752, 382.23144700257757, 319.1889031658204, 215.29828037972797, 194.01820207465443, 186.90061370149866, 169.08319159806732, 150.61337613868906, 132.38988897016011, 131.9955460659564, 127.86187058530126, 126.23607016566524, 123.61112976374547, 112.98930422451737, 108.76594957946611, 106.46957035757814, 95.00069015036746, 93.45200195053364, 92.99649463433371, 85.4123773194858, 79.55753978357082, 75.75133194844871, 75.68717639981493, 71.52757056254462, 68.98374906357547, 67.47580291557198, 67.24908394988536, 532.5136126027999, 612.7051605866528, 886.7088621244504, 2289.5597729721003, 495.91914325134303, 522.9162160081729, 339.4160526906329, 1268.0813298318756, 488.73864002382544, 210.90591512867468, 414.85841347394637, 690.9951406676097, 159.201267438801, 263.97788232456793, 578.7219277221043, 445.0477960541976, 3700.8154783403234, 1945.404767613406, 920.5631995150203, 761.8061428489565, 3529.072844120012, 1450.725489815683, 552.38305087061, 415.3337171130666, 834.621114146164, 884.7303296599384, 903.4193233882941, 2632.1284931746563, 262.3579860708828, 236.2764622652583, 187.4602127414623, 151.47153308183567, 146.42835583303886, 127.59086787660993, 126.5692512650774, 118.41673969665943, 116.5879202993245, 116.34471561434538, 115.3501766871635, 106.38252708399507, 104.83381147262207, 104.18367954075907, 103.44555390722728, 95.31866036439361, 94.20692227280388, 93.26188138042566, 91.73347088919031, 91.26670119161753, 90.8831942974664, 89.64874740349718, 87.46329881982334, 86.48077943674792, 83.45280108678462, 83.44144074111976, 82.040822253228, 81.72358257313135, 81.10575666143623, 80.51563082101525, 754.1297719299887, 346.5780459824209, 261.3705055327176, 227.18456162462437, 115.84481781451781, 119.85351877048929, 145.98871114878145, 186.92061570049205, 256.6218797737273, 351.685583996174, 149.96717477782485, 265.66869637883354, 477.8415904629763, 189.13670311237962, 251.9663716188641, 494.4578342302678, 687.4222226263365, 480.57960946452243, 330.6355719150397, 3529.072844120012, 2771.9344570354247, 3106.1898081135114, 2057.382607155039, 350.1340913952334, 4953.040069355677, 705.0377257897409, 429.9061146684056, 2632.1284931746563, 340.935828672968, 539.4645932481, 2603.293255307125, 797.9571962761618, 921.7958186430802, 483.08954987448084, 430.4109895147165, 390.134687787328, 338.86063586183406, 336.08859133976824, 255.01756523695116, 220.88475113815196, 195.2651667827268, 194.71241185785027, 189.57924510325626, 182.93997334745964, 179.73515684875264, 175.99597679788798, 157.46161189671972, 150.42287288269995, 149.45108721673202, 140.77012162549613, 140.4636859090274, 137.73748417705514, 134.7290329007365, 134.08924284658184, 118.22477910049625, 115.85173485802257, 112.02017889304132, 108.87834310841573, 107.75389090324758, 106.49146222536213, 105.78882075432787, 103.99136910677856, 492.2591080016182, 218.99020947708402, 467.1202599681454, 332.03247771770356, 235.24435358593556, 177.951861391633, 619.7534496306187, 443.6483158558648, 461.07433097593076, 1033.340848173687, 3700.8154783403234, 271.36412662264655, 406.3822230801554, 222.85520540907805, 1319.2436381329344, 807.1747895916504, 497.47292983243665, 457.9850050026447, 343.58836208344763, 315.5054150184656, 314.3200521277282, 313.7439044482487, 267.0239681975907, 263.8206441410848, 167.29151092952984, 156.8341553694168, 143.87605820853858, 135.9946155580244, 131.5759846835688, 131.3010570130922, 125.78494955434469, 113.39055987417457, 113.25713604775976, 112.24632988864164, 109.30477752602567, 106.47968570758061, 102.48464323755347, 97.61091720597943, 88.65073498793642, 86.97919679375882, 83.97685063138347, 81.53998679503401, 81.3124687242117, 80.71616269286653, 884.7303296599384, 1267.9074995774868, 687.4222226263365, 529.8894226455744, 369.1399183479207, 461.07433097593076, 3700.8154783403234, 354.5324680287397, 337.37170769677687, 597.5788563387796, 2415.4478319045343, 218.43441339362758, 348.273734851835, 815.9009389943592, 503.6523317958162, 2028.0177088248483, 596.9707333439634, 1450.725489815683, 1464.5823402625094, 2632.1284931746563, 1419.3233331188164, 1009.8779534951007, 587.2967836937415, 505.39814826089577, 441.1776889558095, 254.67835733271278, 231.0454521304873, 198.34789383064594, 182.62908666968156, 170.38739050991336, 160.88910026629614, 151.58004624350667, 113.66005506186008, 107.79923429281835, 105.11607452817124, 102.61957196933965, 99.72188651034372, 99.41830683043122, 98.14682794921572, 96.68656962261795, 93.6638582696339, 92.06170481688996, 91.35935613949077, 89.92073305938915, 87.02988846904982, 86.78881724086442, 84.46904275057827, 82.41664497018019, 77.98892780671372, 77.88061457534276, 457.68944102379106, 1262.3323772336635, 548.3875823449255, 558.1642914101352, 404.5858732304159, 153.70841079651777, 616.5298872284702, 377.2479517814593, 786.9783780485369, 333.48480532884, 571.9077399994851, 166.46234084178306, 295.0232868381333, 530.4705504729657, 455.71821358911376, 462.24837784437483, 794.6074320268062, 505.8949592899141, 4953.040069355677, 447.6579130563547, 647.1890470191823, 3106.1898081135114, 561.7280138045162, 2603.293255307125, 3529.072844120012, 544.1095969933762, 266.6156117089555, 229.4793890877301, 223.5648141375018, 183.37603254434893, 139.50602652993993, 139.04775561529067, 138.2280999548385, 124.82312754010384, 104.75738758745537, 102.02166897004889, 99.9473437902799, 99.85133634371137, 97.90058770870961, 94.18290409080608, 94.1772033197419, 92.16077269751001, 91.25546519607556, 89.45395798416241, 89.2223490859154, 89.11044377744103, 79.37557943177539, 77.46591811591537, 77.44851772603931, 77.05363574803232, 75.86752444784769, 74.93703543582588, 74.57171522086387, 73.89979953302802, 72.48358543102611, 613.8093672084993, 131.1093070546624, 1033.340848173687, 340.8793015618754, 539.4645932481, 385.9534696299493, 761.8061428489565, 329.3219939616095, 580.7762378721037, 2415.4478319045343, 308.06752783349833, 561.5093756535936, 389.9972582971994, 322.7162271425254, 1162.597910495928, 1450.725489815683, 308.0567681510333, 352.32113890029564, 283.6326844527612, 279.4002602181893, 569.3866071010227, 3529.072844120012, 622.8910774350215, 2028.0177088248483, 391.42591202984005, 345.2251261778942, 251.96308138325654, 235.99732734609307, 190.74779987789668, 182.1417305255863, 164.52118452670564, 164.39275700083112, 154.2503442541508, 152.96112942018186, 140.30339084839653, 140.2616183343562, 138.32113738834812, 125.56558284236385, 123.91091716568307, 121.62712685716058, 118.73085285248266, 112.37772973382677, 110.88107767812102, 110.46761165511138, 109.19347954086778, 98.40808549319541, 97.57989984746678, 97.2234627304007, 90.501406870599, 84.2416227143111, 83.98125671727682, 83.81131687272566, 81.53246488026141, 80.68633661170429, 886.0332760366382, 373.0863167006445, 818.9107462552392, 765.4587038746936, 561.5093756535936, 429.9061146684056, 578.7219277221043, 2028.0177088248483, 209.00315522633434, 705.0377257897409, 540.321141316809, 181.1657618292237, 1267.9074995774868, 1162.597910495928, 360.453290095911, 1450.725489815683, 571.9077399994851], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -5.1758, -5.6911, -5.7965, -5.9553, -6.0526, -6.1324, -6.1725, -6.2203, -6.2554, -6.2841, -6.3598, -6.419, -6.4378, -6.457, -6.5944, -6.6123, -6.6431, -6.691, -6.6977, -6.7427, -6.7446, -6.7582, -6.7597, -6.7668, -6.7804, -6.7994, -6.801, -6.8029, -6.8055, -6.8566, -2.8966, -5.9311, -4.7341, -5.65, -3.5644, -5.5352, -4.8467, -5.3514, -4.819, -5.534, -5.2834, -5.4357, -5.7798, -3.7219, -4.9402, -4.9252, -5.0331, -4.9319, -3.8608, -4.8396, -4.6083, -4.5777, -4.8067, -4.8918, -5.3352, -4.1655, -5.4387, -5.2361, -4.362, -4.8302, -3.8675, -4.1532, -4.8344, -5.0052, -5.0122, -5.179, -4.9544, -4.4251, -4.6041, -5.0931, -5.093, -5.0669, -3.9673, -4.7296, -4.9346, -5.1713, -5.1841, -5.2169, -5.2201, -5.2642, -5.3055, -5.4521, -5.4606, -5.4918, -5.6023, -5.6778, -5.6837, -5.6869, -5.8148, -5.9097, -5.9467, -5.9488, -5.9518, -5.9573, -5.9835, -5.9983, -6.0047, -6.0313, -6.0754, -6.0788, -6.0994, -6.1336, -4.3754, -5.3881, -5.1218, -5.6162, -4.0392, -4.5469, -4.9638, -4.8529, -4.676, -4.1859, -4.7356, -5.3355, -4.0258, -4.9117, -4.9388, -5.4157, -4.633, -5.2582, -4.6125, -4.7153, -4.9104, -4.7764, -4.9105, -4.9444, -5.1587, -4.9746, -5.2079, -3.8292, -4.9417, -5.0246, -5.2064, -5.237, -5.5045, -5.5606, -5.5747, -5.6538, -5.6967, -5.7282, -5.7555, -5.7753, -5.8579, -5.8702, -5.9772, -6.0083, -6.0102, -6.0323, -6.0965, -6.1646, -6.2306, -6.3228, -6.3266, -6.359, -6.3756, -6.3861, -6.4319, -6.4808, -6.4826, -4.5898, -4.8386, -3.32, -5.0038, -5.0288, -2.9289, -3.8664, -4.8203, -4.6745, -4.9763, -4.9078, -3.8086, -3.6908, -4.5538, -5.2525, -4.6747, -4.1488, -4.2687, -5.0017, -5.0489, -4.1871, -4.4253, -4.4175, -5.0713, -5.1292, -4.9888, -5.024, -5.1759, -3.0125, -3.6721, -4.0888, -4.3481, -4.8453, -5.026, -5.4212, -5.5257, -5.5633, -5.664, -5.7803, -5.9101, -5.9131, -5.9452, -5.958, -5.9792, -6.0698, -6.1082, -6.1297, -6.2447, -6.2613, -6.2662, -6.3522, -6.424, -6.4736, -6.4745, -6.5317, -6.5684, -6.5908, -6.5942, -4.6306, -4.5162, -4.1971, -3.347, -4.7681, -4.7227, -5.1331, -4.0619, -4.8719, -5.6264, -5.1143, -4.8304, -5.851, -5.5272, -5.0855, -5.2617, -4.1984, -4.5889, -4.966, -5.0599, -4.3484, -4.9226, -5.2775, -5.378, -5.2405, -5.2396, -5.311, -5.3631, -4.9809, -5.086, -5.3184, -5.5328, -5.5668, -5.7054, -5.7135, -5.7806, -5.7963, -5.7984, -5.8071, -5.8887, -5.9034, -5.9097, -5.9169, -5.9995, -6.0113, -6.0215, -6.0382, -6.0433, -6.0476, -6.0614, -6.0863, -6.0978, -6.1338, -6.1339, -6.151, -6.1549, -6.1626, -6.17, -4.0777, -4.8316, -5.196, -5.3441, -5.882, -5.8591, -5.7181, -5.5436, -5.4118, -5.2229, -5.7637, -5.4444, -5.1399, -5.645, -5.5194, -5.1998, -5.0619, -5.2493, -5.4544, -4.5956, -4.7308, -4.7907, -4.9895, -5.4933, -4.8329, -5.3508, -5.4956, -5.2039, -5.567, -5.5127, -5.3558, -5.5665, -3.6206, -4.2676, -4.3833, -4.4818, -4.623, -4.6313, -4.9081, -5.0524, -5.1762, -5.179, -5.2059, -5.2417, -5.2594, -5.2806, -5.3924, -5.4384, -5.445, -5.5052, -5.5074, -5.5271, -5.5493, -5.5541, -5.6809, -5.7013, -5.7352, -5.7639, -5.7744, -5.7863, -5.7929, -5.8102, -4.6055, -5.2775, -4.9819, -5.175, -5.3781, -5.5311, -5.0431, -5.2391, -5.2392, -5.0133, -4.6909, -5.4193, -5.3591, -5.47, -3.118, -3.6098, -4.0945, -4.1773, -4.4654, -4.5509, -4.5546, -4.5565, -4.7182, -4.7303, -5.1878, -5.2527, -5.3395, -5.3962, -5.4295, -5.4316, -5.4748, -5.5793, -5.5805, -5.5895, -5.6163, -5.6427, -5.6813, -5.7305, -5.8277, -5.8469, -5.8824, -5.9122, -5.915, -5.9225, -3.8614, -3.6701, -4.3325, -4.5378, -4.8086, -4.6753, -3.3099, -4.9004, -5.0035, -4.7433, -4.4299, -5.3387, -5.1851, -4.9533, -5.0941, -4.7564, -5.0869, -4.9104, -5.1012, -5.0702, -3.0218, -3.3624, -3.9052, -4.0556, -4.1917, -4.7427, -4.8405, -4.9937, -5.0767, -5.1464, -5.2041, -5.264, -5.554, -5.6074, -5.6328, -5.657, -5.6859, -5.689, -5.702, -5.7171, -5.7492, -5.7666, -5.7744, -5.7904, -5.8234, -5.8262, -5.8536, -5.8785, -5.9343, -5.9357, -4.3564, -3.4814, -4.2281, -4.2153, -4.631, -5.3839, -4.4368, -4.7812, -4.3098, -4.9003, -4.5587, -5.3747, -5.0738, -4.7698, -4.924, -4.9235, -4.6889, -4.9038, -4.0004, -4.9716, -4.8612, -4.4512, -4.9104, -4.6808, -4.9445, -3.9047, -4.6198, -4.7703, -4.7965, -4.9956, -5.2706, -5.2739, -5.2798, -5.3825, -5.5592, -5.5859, -5.6066, -5.6076, -5.6275, -5.6665, -5.6666, -5.6885, -5.6984, -5.7186, -5.7212, -5.7225, -5.8394, -5.864, -5.8642, -5.8694, -5.8851, -5.8976, -5.9026, -5.9117, -5.9313, -3.9158, -5.4113, -3.7461, -4.8044, -4.5357, -4.7793, -4.3409, -4.9341, -4.6145, -3.7997, -5.0184, -4.7708, -5.0011, -5.1031, -4.5202, -4.4715, -5.1665, -5.1477, -5.2544, -5.2947, -5.1126, -4.7167, -5.2115, -5.111, -4.1125, -4.2384, -4.5543, -4.62, -4.8338, -4.8802, -4.9825, -4.9833, -5.0473, -5.0557, -5.1427, -5.143, -5.157, -5.2544, -5.2678, -5.2865, -5.3108, -5.3662, -5.3797, -5.3835, -5.3952, -5.5001, -5.5086, -5.5123, -5.5847, -5.6571, -5.6602, -5.6623, -5.6902, -5.7007, -3.6871, -4.4254, -3.7838, -3.9437, -4.2741, -4.4711, -4.4523, -3.7993, -5.1471, -4.6633, -4.8227, -5.2238, -4.7243, -5.0468, -5.193, -5.0911, -5.1992], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.7981, 1.7964, 1.796, 1.7952, 1.7947, 1.7942, 1.7939, 1.7936, 1.7934, 1.7932, 1.7926, 1.7921, 1.792, 1.7918, 1.7905, 1.7904, 1.79, 1.7895, 1.7895, 1.789, 1.7889, 1.7888, 1.7888, 1.7887, 1.7885, 1.7883, 1.7883, 1.7882, 1.7882, 1.7876, 1.7361, 1.7482, 1.6591, 1.6893, 1.4925, 1.6783, 1.5843, 1.639, 1.5479, 1.6291, 1.589, 1.5955, 1.6456, 1.2627, 1.4652, 1.4512, 1.466, 1.4409, 1.1865, 1.3887, 1.3124, 1.282, 1.299, 1.3279, 1.4689, 0.9668, 1.4977, 1.3976, 0.9207, 1.172, 0.5366, 0.7175, 1.1253, 1.2246, 1.2132, 1.3483, 1.138, 0.318, 0.5181, 1.2073, 0.8522, -0.3713, 1.9309, 1.9299, 1.9295, 1.9289, 1.9289, 1.9288, 1.9288, 1.9287, 1.9285, 1.928, 1.928, 1.9279, 1.9275, 1.9271, 1.9271, 1.9271, 1.9265, 1.9259, 1.9257, 1.9257, 1.9257, 1.9257, 1.9255, 1.9254, 1.9254, 1.9252, 1.9249, 1.9249, 1.9247, 1.9245, 1.8389, 1.8528, 1.8013, 1.8635, 1.6439, 1.6806, 1.7471, 1.7056, 1.6287, 1.4366, 1.577, 1.6848, 1.0105, 1.4187, 1.1481, 1.629, 0.664, 1.4083, 0.5633, 0.6233, 0.8562, 0.351, 0.3722, 0.1778, 0.8833, -0.2791, -0.4649, 2.1163, 2.1145, 2.1143, 2.1137, 2.1136, 2.1125, 2.1123, 2.1122, 2.1118, 2.1116, 2.1114, 2.1112, 2.1111, 2.1106, 2.1105, 2.1098, 2.1095, 2.1095, 2.1094, 2.1089, 2.1083, 2.1077, 2.1068, 2.1067, 2.1064, 2.1062, 2.1061, 2.1056, 2.105, 2.105, 2.0336, 2.0119, 1.8074, 1.9478, 1.9176, 1.4752, 1.6517, 1.8304, 1.7777, 1.8152, 1.793, 1.3237, 1.1799, 1.2996, 1.6909, 1.2705, 0.8875, 0.7882, 1.408, 1.4389, 0.5084, 0.622, 0.3256, 1.1363, 1.2012, 0.3499, 0.273, 1.028, 2.1225, 2.1222, 2.1218, 2.1215, 2.1205, 2.1201, 2.1187, 2.1182, 2.118, 2.1175, 2.1168, 2.116, 2.116, 2.1158, 2.1157, 2.1155, 2.1148, 2.1145, 2.1143, 2.1133, 2.1131, 2.1131, 2.1122, 2.1114, 2.1108, 2.1108, 2.1101, 2.1096, 2.1093, 2.1092, 2.0036, 1.9778, 1.9273, 1.8287, 1.9373, 1.9298, 1.9516, 1.7047, 1.8481, 1.934, 1.7696, 1.5433, 1.9907, 1.8088, 1.4655, 1.552, 0.4972, 0.7497, 1.1209, 1.2162, 0.3947, 0.7095, 1.3201, 1.5048, 0.9444, 0.8869, 0.7946, -0.3268, 2.3612, 2.3609, 2.3599, 2.3587, 2.3585, 2.3576, 2.3575, 2.357, 2.3569, 2.3569, 2.3568, 2.3562, 2.356, 2.356, 2.3559, 2.3552, 2.3551, 2.355, 2.3548, 2.3547, 2.3547, 2.3546, 2.3543, 2.3542, 2.3538, 2.3538, 2.3536, 2.3536, 2.3535, 2.3534, 2.2086, 2.2322, 2.1499, 2.142, 2.2776, 2.2664, 2.2102, 2.1376, 1.9525, 1.8263, 2.1377, 1.8852, 1.6026, 2.0244, 1.8631, 1.5086, 1.317, 1.4875, 1.6564, 0.1475, 0.2538, 0.08, 0.2931, 1.5602, -0.4288, 1.0028, 1.3527, -0.1676, 1.5131, 1.1086, -0.3085, 0.6633, 2.4649, 2.464, 2.4638, 2.4636, 2.4633, 2.4632, 2.4624, 2.4619, 2.4613, 2.4613, 2.4612, 2.461, 2.4609, 2.4608, 2.4602, 2.46, 2.4599, 2.4596, 2.4596, 2.4594, 2.4593, 2.4593, 2.4584, 2.4582, 2.4579, 2.4577, 2.4576, 2.4575, 2.4575, 2.4573, 2.1073, 2.2454, 1.7834, 1.9316, 2.0731, 2.1993, 1.4394, 1.5777, 1.5391, 0.958, 0.0047, 1.8891, 1.5454, 2.0354, 2.609, 2.6086, 2.6079, 2.6077, 2.6071, 2.6068, 2.6068, 2.6068, 2.6063, 2.6063, 2.6043, 2.6039, 2.6034, 2.603, 2.6028, 2.6028, 2.6025, 2.6017, 2.6017, 2.6016, 2.6014, 2.6012, 2.6009, 2.6004, 2.5995, 2.5993, 2.5989, 2.5986, 2.5986, 2.5985, 2.2651, 2.0966, 2.0464, 2.1014, 2.1921, 2.103, 1.3857, 2.1407, 2.0872, 1.7756, 0.6923, 2.1867, 1.8737, 1.2543, 1.5959, 0.5406, 1.4331, 0.7216, 0.5213, -0.0339, 2.6321, 2.6318, 2.6312, 2.6309, 2.6307, 2.6291, 2.6288, 2.6281, 2.6277, 2.6274, 2.627, 2.6267, 2.6247, 2.6242, 2.624, 2.6238, 2.6235, 2.6235, 2.6234, 2.6233, 2.623, 2.6228, 2.6227, 2.6225, 2.6222, 2.6222, 2.6219, 2.6216, 2.621, 2.621, 2.4293, 2.2898, 2.3768, 2.3719, 2.278, 2.4929, 2.0509, 2.1977, 1.9338, 2.2019, 2.0042, 2.4224, 2.151, 1.8683, 1.866, 1.8523, 1.5451, 1.7817, 0.4037, 1.8363, 1.5781, 0.4195, 1.6704, 0.3666, -0.2014, 2.708, 2.7063, 2.7057, 2.7056, 2.7048, 2.7032, 2.7032, 2.7031, 2.7024, 2.7011, 2.7008, 2.7006, 2.7006, 2.7004, 2.7001, 2.7001, 2.6999, 2.6998, 2.6996, 2.6995, 2.6995, 2.6983, 2.698, 2.698, 2.6979, 2.6978, 2.6976, 2.6975, 2.6974, 2.6972, 2.5763, 2.6245, 2.2252, 2.2759, 2.0856, 2.1768, 1.9353, 2.1807, 1.933, 1.3225, 2.1632, 1.8105, 1.9446, 2.032, 1.3332, 1.1606, 2.0151, 1.8996, 2.0097, 1.9846, 1.4547, 0.0264, 1.266, 0.186, 2.8296, 2.8292, 2.8283, 2.828, 2.8271, 2.8269, 2.8264, 2.8264, 2.826, 2.8259, 2.8254, 2.8254, 2.8253, 2.8246, 2.8245, 2.8244, 2.8242, 2.8238, 2.8237, 2.8236, 2.8235, 2.8226, 2.8226, 2.8225, 2.8218, 2.8211, 2.821, 2.821, 2.8207, 2.8206, 2.438, 2.5646, 2.4201, 2.3277, 2.3071, 2.3772, 2.0988, 1.4977, 2.4224, 1.6903, 1.797, 2.4887, 1.0424, 0.8066, 1.8315, 0.541, 1.3637]}, \"token.table\": {\"Topic\": [1, 5, 6, 8, 2, 7, 9, 9, 8, 6, 1, 6, 4, 2, 3, 1, 10, 6, 9, 2, 4, 4, 7, 5, 5, 7, 7, 3, 3, 4, 5, 6, 8, 9, 6, 7, 10, 1, 8, 4, 8, 8, 3, 5, 8, 10, 1, 5, 9, 9, 10, 9, 6, 1, 6, 1, 1, 2, 3, 4, 5, 7, 9, 10, 4, 5, 6, 3, 10, 1, 2, 7, 6, 3, 8, 3, 6, 4, 4, 7, 2, 4, 2, 4, 3, 4, 10, 7, 4, 6, 6, 8, 4, 6, 1, 2, 5, 5, 6, 7, 9, 4, 3, 5, 7, 8, 9, 2, 1, 2, 10, 1, 5, 9, 9, 2, 2, 2, 4, 7, 4, 5, 2, 2, 8, 2, 5, 4, 7, 2, 1, 1, 2, 8, 4, 7, 4, 8, 2, 10, 4, 5, 6, 9, 5, 4, 5, 9, 10, 1, 10, 4, 9, 4, 6, 9, 2, 3, 6, 1, 4, 7, 1, 6, 8, 9, 4, 7, 6, 7, 7, 10, 6, 3, 8, 4, 6, 9, 6, 1, 3, 7, 5, 9, 10, 5, 5, 10, 4, 4, 1, 2, 5, 6, 5, 8, 1, 5, 1, 4, 5, 10, 2, 3, 4, 10, 1, 10, 2, 4, 10, 2, 4, 7, 2, 4, 7, 2, 1, 2, 4, 7, 1, 5, 10, 9, 9, 3, 6, 2, 5, 6, 2, 2, 2, 5, 5, 9, 10, 10, 8, 7, 8, 10, 2, 3, 6, 7, 9, 10, 7, 5, 6, 9, 9, 2, 4, 5, 2, 3, 4, 6, 8, 1, 2, 3, 3, 4, 3, 1, 7, 10, 7, 3, 4, 9, 4, 9, 10, 1, 3, 9, 1, 3, 4, 5, 8, 7, 5, 9, 10, 2, 1, 2, 3, 4, 7, 10, 4, 10, 10, 8, 3, 7, 1, 2, 3, 5, 3, 3, 6, 1, 3, 5, 8, 6, 6, 7, 1, 4, 5, 6, 10, 3, 3, 5, 7, 9, 2, 6, 3, 10, 2, 3, 9, 1, 6, 7, 1, 1, 3, 4, 7, 1, 6, 1, 2, 4, 6, 9, 10, 1, 2, 3, 4, 5, 8, 9, 7, 5, 2, 5, 8, 4, 5, 7, 8, 1, 9, 7, 4, 7, 10, 8, 3, 2, 5, 9, 9, 1, 4, 5, 8, 10, 7, 5, 4, 5, 4, 5, 10, 6, 1, 5, 6, 7, 10, 1, 2, 5, 1, 1, 2, 3, 1, 3, 4, 7, 9, 1, 1, 2, 3, 4, 7, 8, 9, 10, 5, 3, 1, 2, 4, 9, 2, 1, 2, 3, 2, 4, 8, 10, 2, 2, 8, 3, 8, 3, 5, 8, 5, 1, 2, 3, 5, 8, 9, 8, 4, 6, 10, 3, 2, 1, 2, 3, 4, 5, 6, 8, 9, 4, 5, 10, 6, 4, 4, 6, 9, 6, 7, 6, 6, 1, 4, 5, 3, 9, 3, 2, 1, 2, 3, 8, 6, 5, 3, 8, 1, 4, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 6, 9, 1, 3, 4, 5, 8, 2, 5, 3, 1, 3, 5, 6, 1, 2, 5, 1, 4, 6, 7, 3, 1, 2, 3, 4, 6, 7, 10, 6, 9, 3, 2, 3, 4, 7, 10, 1, 1, 5, 8, 9, 4, 8, 1, 2, 4, 1, 2, 3, 4, 5, 6, 8, 9, 1, 2, 4, 5, 4, 5, 6, 9, 1, 2, 3, 4, 5, 6, 7, 8, 1, 4, 10, 6, 7, 5, 1, 2, 6, 5, 6, 1, 2, 2, 2, 7, 6, 5, 1, 2, 4, 7, 8, 3, 7, 10, 3, 10, 5, 9, 10, 8, 9, 1, 2, 8, 10, 1, 4, 5, 1, 4, 5, 9, 3, 4, 5, 4, 9, 8, 8, 1, 6, 7, 4, 7, 4, 10, 3, 3, 8, 5, 2, 9, 2, 4, 9, 1, 8, 1, 7, 1, 8, 3, 3, 4, 7, 1, 5, 6, 8, 8, 1, 4, 7, 8, 10, 4, 6, 9, 1, 4, 6, 1, 4, 5, 6, 4, 7, 5, 7, 4, 6, 1, 1, 2, 4, 8, 5, 6, 9, 10, 9, 1, 1, 2, 4, 7, 10, 2, 5, 6, 4, 1, 3, 4, 5, 6, 3, 2, 1, 3, 6, 5, 3, 9, 2, 8, 4, 5, 7, 9, 10, 1, 5, 2, 8, 8, 4, 1, 7, 7, 8, 5, 1, 4, 7, 9, 2, 1, 3, 2, 4, 5, 9, 5, 10, 1, 2, 3, 3, 4, 3, 5, 6, 2, 6, 2, 4, 5, 6, 7, 2, 5, 1, 3, 8, 4, 1, 6, 7, 9, 10, 5, 2, 7, 7, 2, 4, 7, 8, 7, 9, 9, 2, 3, 10, 1, 9, 1, 3, 10, 1, 3, 8, 9, 10, 5, 2, 1, 2, 3, 5, 8, 9, 3, 8, 7, 2, 3, 4, 7, 9, 10, 2, 10, 3, 4, 6, 7, 8, 2, 3, 4, 8, 9, 2, 8, 10, 1, 7, 4, 5, 6, 9, 4, 7, 10, 9, 1, 2, 5, 3, 2, 7, 5, 10, 9, 1, 5, 8, 6, 8, 1, 4, 8, 4, 8, 9, 8, 1, 4, 5, 5, 1, 2, 8, 5, 1, 3, 5, 7, 8, 1, 2, 3, 5, 9, 10, 2, 10, 5, 1, 3, 4, 5, 6, 10, 10, 2, 1, 2, 3, 1, 2, 3, 5, 1, 2, 3, 4, 7, 9, 3, 8, 1, 5, 2, 3, 4, 5, 6, 7, 10, 2, 2, 7, 10, 6, 4, 10, 2, 4, 2], \"Freq\": [0.8155135577190413, 0.05306423834473214, 0.01954998254805921, 0.10892133133918702, 0.13495131931207807, 0.47950787925780936, 0.38475482527273325, 0.9885652727677435, 0.9965553862139201, 0.9930035853283061, 0.9968915091848665, 0.9925434393851525, 0.9947520280892891, 0.7389074671407423, 0.25762674141688713, 0.9906879197419644, 0.9903197216915502, 0.994340911559406, 0.9934056488062328, 0.7973002368206211, 0.20019547431085463, 0.8731086268106085, 0.11934578352087455, 0.9949572794693045, 0.9970746387842578, 0.9978492636398892, 0.9958002929854701, 0.997455335359589, 0.11817351352220279, 0.12579761116879654, 0.07624097646593729, 0.08767712293582788, 0.4968370299696913, 0.0953012205824216, 0.8036888974181164, 0.19635581016465345, 0.9907101970913526, 0.5472929057084656, 0.451237416135143, 0.29659957981691254, 0.7019523389000263, 0.9883151807024357, 0.9954450480487799, 0.9944406209116147, 0.9928990176681443, 0.9960796408746241, 0.6983028194518477, 0.1734913837147448, 0.12722701472414616, 0.4060484292619502, 0.5912635022586292, 0.9979109710478319, 0.9953849612439157, 0.893135429181259, 0.10261555994848508, 0.9924850779987641, 0.07237757986408609, 0.08340654441480397, 0.03997999649635232, 0.2433265304002132, 0.09512481924994172, 0.15164826257237085, 0.2123075676013192, 0.10132861180972053, 0.9900816008230718, 0.8562305880802576, 0.13699689409284121, 0.9824790717379042, 0.9926486125151904, 0.06123200113216754, 0.9359748744488467, 0.9900910393054759, 0.996944575325528, 0.9995956603695229, 0.9897606143982274, 0.4126102390395629, 0.5872919460782099, 0.999627959634626, 0.9909208345125079, 0.9926595646609702, 0.1659507746790617, 0.8297538733953085, 0.254634103412466, 0.7451214072412814, 0.9935081566958007, 0.9834640205107184, 0.9910052489815941, 0.987736546360494, 0.7311218587726274, 0.2651737311610566, 0.9896402504634402, 0.989382456173512, 0.993969852534641, 0.99904512309228, 0.2213119475020952, 0.7777176912220521, 0.9861657690178135, 0.17692432070232197, 0.2205143707304303, 0.13333427067421366, 0.46410582676985906, 0.9962752365322588, 0.07855434145508108, 0.35058511649397295, 0.5687916205358649, 0.9973364154700175, 0.9924647786607248, 0.9934370698042504, 0.12032373259539635, 0.877360550174765, 0.9877401889405489, 0.8847480071149012, 0.11394481909813121, 0.9923333502632925, 0.9939855083726213, 0.9943489637203796, 0.9982083475596694, 0.23939175342120766, 0.7590969275151115, 0.9889001603640266, 0.9955896285107465, 0.9945741655056762, 0.9977795106889152, 0.22753157440285138, 0.7703824960096542, 0.911812522144781, 0.08760309752676516, 0.9997548162804002, 0.9937595908165026, 0.9937734001651513, 0.9875178145727764, 0.30167110180977175, 0.07795994765870506, 0.6169004553862748, 0.8882402042045245, 0.11079528974221342, 0.9929578183022574, 0.9991306340612129, 0.39453467374699513, 0.603559666460635, 0.25718773495928937, 0.6624532567133211, 0.07793567726039072, 0.9979607104901154, 0.9975450111000513, 0.2726853892015024, 0.21690883231937694, 0.5081864071482546, 0.9944596787173052, 0.9941170305419333, 0.9883157652596195, 0.998692529149065, 0.9924961156305083, 0.16161172791645043, 0.22161129157405476, 0.6164471298369996, 0.20598558519179624, 0.7336415892790642, 0.060339211823859505, 0.849937492233624, 0.14746549549794738, 0.9968893861822051, 0.8060937820690702, 0.01992246989904546, 0.0628324050662203, 0.11187233097156299, 0.29048399425707766, 0.7086905229540378, 0.3968991282005527, 0.6029396592336265, 0.9933776443158941, 0.9961776885011514, 0.994861848232183, 0.9845020090780755, 0.9973305790721265, 0.9892846000459268, 0.9966988912043238, 0.9874403523418053, 0.9971887727272055, 0.7051730955198348, 0.12188176959602082, 0.1726658402610295, 0.25828708514067433, 0.44561617985808644, 0.29518524016077063, 0.9920463497328834, 0.36286992596121975, 0.6350223704321346, 0.9902082648482079, 0.992991993152534, 0.17844419963125044, 0.2752274943465049, 0.49298990745582744, 0.05141612531747894, 0.9863664836264083, 0.9972335706695664, 0.991650336255001, 0.9882949751275608, 0.993880546026094, 0.8307805931806691, 0.16736599328639695, 0.9958531304501028, 0.5934664032957634, 0.16679121138377664, 0.24048965362311983, 0.9989118961807322, 0.9878242548208739, 0.9920538499753879, 0.7387918620376998, 0.1863862467298036, 0.07428437369666085, 0.8312232972190294, 0.10339606867846464, 0.06284859076534126, 0.13276296325955214, 0.20601149471309813, 0.6546587498660674, 0.9910771144431038, 0.32350008335964037, 0.3007352626787768, 0.30792415342010215, 0.0670963135857032, 0.2855077788255342, 0.7137694470638354, 0.9954957176197406, 0.9976910140219863, 0.993328345608865, 0.9908211890434774, 0.9919328023981673, 0.12118482542927811, 0.8771473078690606, 0.9970915485783552, 0.9954795891776442, 0.9897256846866261, 0.19794499317981115, 0.79712983739978, 0.5829070321012368, 0.2587538532742076, 0.15638969153935622, 0.9904487671711453, 0.9884674651745622, 0.6582869744554918, 0.14628599432344264, 0.19233899253637826, 0.6097301431614559, 0.1529446271759979, 0.029359906109678167, 0.12358472106631974, 0.052574715591749276, 0.030725483138035293, 0.9888964753691447, 0.1574801682920392, 0.3543303786570882, 0.4831777890778476, 0.9875385675307101, 0.2964866952318048, 0.701444132621587, 0.9884419708514929, 0.09564471327199482, 0.3045529027871414, 0.15605190060167576, 0.1044540947575733, 0.3372734625907186, 0.9967568849241053, 0.5986730469114631, 0.40050118184586303, 0.9965907238943496, 0.9894664959929915, 0.9932643515247961, 0.2334555163516566, 0.5986241111854979, 0.16720462657618648, 0.9838589487589756, 0.8998042436904222, 0.09790616123056407, 0.987404915740888, 0.40430233188742776, 0.46074713796262057, 0.13389233068999232, 0.45908853735359073, 0.49209490278424106, 0.04800925880821864, 0.3386785949951054, 0.3917983366055544, 0.058270746857522884, 0.10173235362970846, 0.10945886150031922, 0.988367620075783, 0.35962004941989684, 0.4971218330216221, 0.14102747036074387, 0.9950303474986669, 0.19995838731145885, 0.2703807499378595, 0.17065857220412425, 0.25341769908624473, 0.04163657936305441, 0.06425398049854075, 0.5183836755258677, 0.48036887265397077, 0.9915278688292569, 0.9961733337739266, 0.9984227920106564, 0.9880629414783356, 0.09786625354959012, 0.13927120697441672, 0.14303529364940096, 0.6173102146974146, 0.9918299565301286, 0.9195214384090317, 0.07802000083470571, 0.6681313270299843, 0.19330765922496737, 0.08445480257401487, 0.05254965493494259, 0.9991366649457667, 0.6988189642574764, 0.3006546706689142, 0.06382637177265046, 0.15460165607153115, 0.2567238509077719, 0.20566275348965152, 0.31913185886325235, 0.9953660797187387, 0.5076628118371314, 0.14272948064430532, 0.21247229505004542, 0.13624177697865508, 0.9950758188694946, 0.9960098229468795, 0.9920452105912193, 0.9874159724819374, 0.1560365154502946, 0.7384263266380138, 0.1032917778332936, 0.6361750947824493, 0.24130779457265314, 0.1214976308337834, 0.994901404213067, 0.3874683468166773, 0.42894914306955756, 0.1272706248667918, 0.05656472216301858, 0.9875718777056129, 0.9945292252602796, 0.17499046614834615, 0.14448754085643262, 0.23599631673217325, 0.1284333696501623, 0.23599631673217325, 0.07706002179009738, 0.5088852739046404, 0.07748065260578173, 0.09852478047401873, 0.09182892160685241, 0.05165376840385448, 0.1061771906079231, 0.06600203740492518, 0.9956224178374647, 0.9947095743170429, 0.9972185189411236, 0.9870041902378998, 0.9932043955465601, 0.5600618256535022, 0.1635322643381027, 0.1331413125584553, 0.14327162981833774, 0.996216724110153, 0.9911154102874904, 0.9990493355435522, 0.539325344344775, 0.2720703746024981, 0.187800789548627, 0.9944471638921816, 0.9966042912033527, 0.9949093743379851, 0.9920043264243613, 0.9914739614422298, 0.9908009979328297, 0.2535529142282296, 0.24800066063199103, 0.1184480767197569, 0.024059765583700623, 0.3553442301592707, 0.996165257356849, 0.9871886030910884, 0.8426236700733776, 0.15615054025835318, 0.4688540918992077, 0.1609085049121541, 0.36897984747097406, 0.9918767320669556, 0.38563722419366475, 0.14360549352818477, 0.35820696138491037, 0.0177489935821352, 0.09358560252398558, 0.6136433612418731, 0.25668140597673555, 0.12880070550803935, 0.9810380022447245, 0.9919739009576467, 0.9972828014840294, 0.9919105441911714, 0.20604025143914334, 0.07873082778568892, 0.3366999230834781, 0.3082228151609949, 0.07035520780848796, 0.993292481682279, 0.9915222888323132, 0.04215062261859951, 0.144014627280215, 0.1527960069924232, 0.11591421220114866, 0.12293931597091523, 0.2845167026755467, 0.13523324756800675, 0.9948239194422086, 0.9874746269928191, 0.5155854951680978, 0.26216211618716834, 0.22174545660831324, 0.9863259437688607, 0.9958948791888783, 0.4345101558002104, 0.11291409246011243, 0.4524927705253394, 0.16611070869592626, 0.06819281725411709, 0.5333028016027106, 0.23080645839855016, 0.997301556019611, 0.22429391904544496, 0.7749993137749115, 0.750268303932809, 0.2481803549904966, 0.7226265102732367, 0.27499953307620395, 0.988166266932353, 0.9953690425777074, 0.5330257053237559, 0.06088870416619684, 0.14174091789508117, 0.10780295163851245, 0.08384674016328746, 0.07286680990380934, 0.9886927628891418, 0.23039938823549586, 0.764251629268962, 0.9968321129694939, 0.9911524662925836, 0.9976648059695875, 0.4150905121050461, 0.21046158283546249, 0.09526667490935485, 0.02527483211880843, 0.12588810613021892, 0.06318708029702107, 0.03207959461233378, 0.03305170353998026, 0.1681652416786192, 0.15575035806476142, 0.6749182182806328, 0.995994512370849, 0.9982776817551895, 0.981426007961444, 0.9945888953179555, 0.9878240598930736, 0.9974602070268647, 0.98874217249809, 0.9963412098332469, 0.9904668135894948, 0.33085049150787527, 0.2517793048581944, 0.41616414026147835, 0.8434141352150957, 0.1547548871954304, 0.9957058400093696, 0.9942239571324066, 0.20924818522165423, 0.22621425429368025, 0.09802617686059477, 0.46562434008782516, 0.9908929006976912, 0.9968869854800974, 0.8184857169124168, 0.1796050162503736, 0.9962275058384856, 0.9912442666028676, 0.1432114259640003, 0.855290460618335, 0.22697179553394353, 0.09095873453982006, 0.16661599971780125, 0.1776670609235738, 0.10881044879529878, 0.04023719721076153, 0.03938711557954826, 0.058655632553715746, 0.0682898910407995, 0.021818761867807308, 0.22023380196969772, 0.18914197110338746, 0.5881538005543692, 0.5626868234228035, 0.12532000521666004, 0.07519200312999602, 0.18296720761632362, 0.05388760224316381, 0.9931465695047169, 0.9927634526718231, 0.996140870404529, 0.5527379465862532, 0.2568684861264583, 0.13448611839081587, 0.05513930854023451, 0.1666889106278481, 0.22225188083713082, 0.6072238887157324, 0.2819405193532742, 0.23428860058934053, 0.12111529352499807, 0.36334588057499423, 0.995432538565802, 0.11402892213076539, 0.10970555067557049, 0.20022614051871362, 0.1967134012113678, 0.0853865862400992, 0.29398925895325295, 0.9890700475350169, 0.12381694392452021, 0.8748644590456232, 0.9954941498514612, 0.1654612631852031, 0.37504552988646034, 0.07108706122030947, 0.2573841871769826, 0.1286920935884913, 0.9933557595616558, 0.09824555802798587, 0.28546822521339293, 0.07970866028685646, 0.5357163447186399, 0.9929485401430893, 0.9957924567037267, 0.1748929351996903, 0.45624243965136596, 0.36716653476705163, 0.5840686441524001, 0.0887466871287773, 0.09127199936414901, 0.01226580228609117, 0.12121498729784216, 0.022367051227578018, 0.03679740685827351, 0.043291066892086484, 0.5552505418426116, 0.3144340147063778, 0.05490117717095485, 0.073617487570144, 0.24875730848005778, 0.4247075998440011, 0.23864522276948633, 0.08494151996880021, 0.16617724966626843, 0.3500108321095779, 0.10074495761017524, 0.15267534813088413, 0.10489938885183195, 0.06958672329774991, 0.04050570460615293, 0.015579117156212666, 0.9504183478597503, 0.048054860285043556, 0.9957669795870006, 0.9977446213134523, 0.9983980781488773, 0.9911459758572888, 0.9376306374480574, 0.062170367029708816, 0.9959097771318537, 0.3230683280661057, 0.6758929495067212, 0.8676524727419161, 0.13132835046603833, 0.9955156990312621, 0.9961184882276688, 0.9982875960062212, 0.9926480612563431, 0.996480736610998, 0.9873032584068466, 0.10476483117047071, 0.35382839206630673, 0.11464830580919436, 0.42696610439286176, 0.9940256582168393, 0.9946812901344609, 0.993731636773782, 0.3370330664997476, 0.6630759243092861, 0.1850243691727447, 0.577795398469273, 0.23371499263925646, 0.9929123326553255, 0.992770079467445, 0.9937558581583179, 0.3033105188795212, 0.5595835776119509, 0.13786841767250965, 0.7152610672702515, 0.16405070350235126, 0.11975701355671642, 0.9860346734060144, 0.9895015474295519, 0.9094434699804436, 0.08343518073215078, 0.066025613240324, 0.132051226480648, 0.8011107739825978, 0.07627223592777267, 0.915266831133272, 0.9949446501937693, 0.9925859000941754, 0.7761486468589043, 0.07761486468589043, 0.14373123089979709, 0.8221413263575618, 0.17705924312501672, 0.9926242348454529, 0.9852611728703063, 0.9934719861669989, 0.9955837397193434, 0.9954751235272233, 0.9864694839762206, 0.49666170595215603, 0.4999078608930198, 0.7818083177426627, 0.21548073775239465, 0.9826699919342607, 0.28993948551178744, 0.7097972104332283, 0.9909524448766954, 0.995495049554354, 0.9918653704722054, 0.9941927261824672, 0.9950558285417298, 0.6523343030354016, 0.19697937777931734, 0.14837407676883643, 0.7188456576530041, 0.1891699099086853, 0.0934721907784092, 0.9873196383829682, 0.9851207780250196, 0.8101165176313488, 0.05241930408202845, 0.13581365148525554, 0.9918574343690495, 0.9938444571488869, 0.27869101926904727, 0.07040615223639089, 0.6483233185100995, 0.12202425132210501, 0.37035430664428365, 0.5052232160002944, 0.07382212679633567, 0.3321995705835105, 0.19439826723035059, 0.3986394847002126, 0.9951813229306082, 0.9952710647932872, 0.9150171928253139, 0.07769013901347005, 0.9935937357946129, 0.9935207758579155, 0.9816807793659157, 0.6619267832484552, 0.10135362843690902, 0.18143550769570133, 0.05505629199041971, 0.46668185534862583, 0.22810906830941802, 0.15695578094684726, 0.14649206221705743, 0.9905215711158095, 0.9907839951651507, 0.10335968497729209, 0.18874377256722902, 0.5639843680282677, 0.14155782942542178, 0.9934692900423973, 0.08417170083961943, 0.8072831307799864, 0.10712761925042473, 0.9967782687368231, 0.6239936369036285, 0.08435699266291599, 0.06326774449718699, 0.15630854522834434, 0.07195155256542834, 0.9953969678854236, 0.9965087204430972, 0.32060243585910836, 0.11423764955899263, 0.5601329913860283, 0.9947029345328291, 0.9962628880310169, 0.9874956964820375, 0.34784193506392486, 0.6507043095592387, 0.2694205095983614, 0.17068877005610475, 0.43508902171163955, 0.12550644857066526, 0.9964512253454493, 0.8574315859946153, 0.1408341735450093, 0.1813456736391819, 0.814963087559215, 0.9939624385733673, 0.9950560296235986, 0.372321329930492, 0.6261767821558275, 0.9922798776677086, 0.9977919448399061, 0.9873133615114755, 0.2019289221999979, 0.19438017744485778, 0.6020123942224236, 0.9963727263794491, 0.9954422955780622, 0.9893504532767073, 0.9914500347801405, 0.17402405435936552, 0.824223817899852, 0.4068966010682571, 0.5890891090092678, 0.2870299524311769, 0.7120550743004196, 0.249550721236342, 0.7501030399952838, 0.987157664732997, 0.13383272293885473, 0.8650163799706464, 0.23782258037326076, 0.11218046244021734, 0.6506466821532606, 0.9958722537671391, 0.996760998832395, 0.06987516664003628, 0.14425840854717167, 0.1487664838142708, 0.412488886939569, 0.22314972572140618, 0.5719551411155664, 0.4252999767269597, 0.33824198781391174, 0.27949469519360076, 0.38274751252626854, 0.9951632716143106, 0.10158817829078047, 0.18767985480839103, 0.25138769543142286, 0.4597295526040404, 0.9940571793128169, 0.9956928655664905, 0.9232051656171372, 0.0723407032759697, 0.997628943741371, 0.3414607484658795, 0.6584751154018692, 0.9985445660508738, 0.9927610022673524, 0.9990573097364377, 0.9974736000399669, 0.986299967458396, 0.7011526813354028, 0.1388690747499244, 0.15929099750726625, 0.9941923023286001, 0.9899857649814685, 0.7348868974127145, 0.26486952555825016, 0.9937165120065135, 0.28585156200042533, 0.18850751656244266, 0.34765730513565246, 0.08189260965417591, 0.09425375828122133, 0.9902820944587879, 0.9904684677706649, 0.2824527927112027, 0.010094810318484729, 0.5263434100057938, 0.061174550530017455, 0.1076106779950472, 0.012113772382181674, 0.7122133845344588, 0.28645065795561747, 0.9911273942049054, 0.2815557268140773, 0.15828264151894714, 0.09023589843603529, 0.1262316393422133, 0.08037405161242488, 0.26331131019039805, 0.23319000484760932, 0.7665786366254743, 0.09435655349682778, 0.20846215307438695, 0.07460750741609638, 0.15799236864585117, 0.4651997521238951, 0.15930973961149486, 0.16836142936214799, 0.4489638116323946, 0.139396022160058, 0.08327554570600869, 0.18820513051090199, 0.6467894626008462, 0.1643481421362806, 0.9912059304101234, 0.993741302474494, 0.9932593619868382, 0.9935958916826785, 0.9970684162878856, 0.994208827499761, 0.3645830316943767, 0.5928179377144337, 0.04149725564001036, 0.9949252331099449, 0.7392985887859335, 0.15751378867604082, 0.10162179914583279, 0.9888850378362376, 0.9963489439332138, 0.9926863607508046, 0.8001751061709272, 0.2000437765427318, 0.9862423012870735, 0.990673552328579, 0.12361067232132518, 0.8717805311082935, 0.9969817066899095, 0.9944738315720297, 0.605477418778762, 0.2645504626839564, 0.1284010613863554, 0.9959274789901512, 0.9990676309703804, 0.9875001244648859, 0.990911072809355, 0.40841495733867506, 0.14280243263590037, 0.44839963847672715, 0.9886385320044683, 0.14927039943714032, 0.3894010420099313, 0.45862789392280795, 0.9955024521407241, 0.5412375256331897, 0.22433123844555203, 0.06914318993184823, 0.061460613272753976, 0.10371478489777233, 0.9834449324937199, 0.06967155133235008, 0.44125315843821716, 0.10321711308496308, 0.25288192705815954, 0.13160181918332794, 0.3301385566417804, 0.6650617300464852, 0.9861208833552771, 0.5954928659588394, 0.1627218877910782, 0.0926129893279009, 0.049335891511124774, 0.09867178302224955, 0.9914937690750886, 0.9918940585825166, 0.999342454368099, 0.7154838167607895, 0.02688665699727393, 0.25691694464061754, 0.23433800267639496, 0.1371584635087561, 0.6279766423427802, 0.9945975902422931, 0.27738127528579987, 0.17305279562606618, 0.09397843207444263, 0.05837426838104146, 0.14697067571113276, 0.24964314775721985, 0.18622830751529845, 0.8109942424053319, 0.841313229846639, 0.15615283432759589, 0.3981568539368642, 0.2925389098581922, 0.08624199030884366, 0.07940341839008073, 0.04635032078272655, 0.07104516382270382, 0.02621452568859125, 0.9927599515590487, 0.9980786031141664, 0.9939110216150849, 0.9957739888103458, 0.9946457263869642, 0.9857394085283934, 0.9948438570131063, 0.9967570418325754, 0.9924577298581049, 0.9976509367079591], \"Term\": [\"ability\", \"ability\", \"ability\", \"ability\", \"access\", \"access\", \"access\", \"access_control\", \"account\", \"accounting\", \"ad\", \"addiction\", \"advertising\", \"advice\", \"advice\", \"african\", \"air\", \"algorithm\", \"alternative\", \"amazing\", \"amazing\", \"amazon\", \"amazon\", \"annual\", \"anxiety\", \"app\", \"apple\", \"art\", \"article\", \"article\", \"article\", \"article\", \"article\", \"article\", \"artificial_intelligence\", \"artificial_intelligence\", \"asset\", \"attention\", \"attention\", \"audience\", \"audience\", \"authentic\", \"author\", \"authority\", \"automation\", \"award\", \"bad\", \"bad\", \"bad\", \"bank\", \"bank\", \"banking\", \"battle\", \"behavior\", \"behavior\", \"belief\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big\", \"big_tech\", \"bill\", \"bill\", \"birthday\", \"bitcoin\", \"black\", \"black\", \"blockchain\", \"body\", \"book\", \"bot\", \"brain\", \"brain\", \"brand\", \"bright\", \"budget\", \"building\", \"building\", \"business\", \"business\", \"busy\", \"buyer\", \"calendar\", \"camera\", \"campaign\", \"campaign\", \"cancer\", \"candidate\", \"capital\", \"care\", \"career\", \"career\", \"caregiver\", \"case\", \"case\", \"case\", \"case\", \"ceo\", \"change\", \"change\", \"change\", \"channel\", \"chart\", \"chicken\", \"child\", \"child\", \"chinese\", \"choice\", \"choice\", \"citizen\", \"claim\", \"clarity\", \"class\", \"client\", \"client\", \"cliq\", \"cloud\", \"cold\", \"college\", \"comment\", \"comment\", \"community\", \"community\", \"company\", \"computer\", \"confidence\", \"confident\", \"connection\", \"connection\", \"connection\", \"consumer\", \"consumer\", \"contact\", \"content\", \"conversation\", \"conversation\", \"cost\", \"cost\", \"cost\", \"country\", \"creator\", \"crisis\", \"crisis\", \"crisis\", \"crypto\", \"curiosity\", \"currency\", \"customer\", \"data\", \"datum\", \"datum\", \"datum\", \"day\", \"day\", \"day\", \"decision\", \"decision\", \"device\", \"different\", \"different\", \"different\", \"different\", \"digital\", \"digital\", \"digital_health\", \"digital_health\", \"disability\", \"discussion\", \"disease\", \"disruption\", \"distraction\", \"diverse\", \"doctor\", \"driver\", \"drug\", \"easy\", \"easy\", \"easy\", \"economic\", \"economic\", \"economic\", \"economist\", \"economy\", \"economy\", \"ecosystem\", \"edge\", \"effort\", \"effort\", \"effort\", \"effort\", \"election\", \"email\", \"emotion\", \"emotional_intelligence\", \"empathy\", \"employee\", \"employee\", \"entertainment\", \"entrepreneur\", \"entrepreneur\", \"entrepreneur\", \"episode\", \"equal\", \"et\", \"event\", \"event\", \"event\", \"excited\", \"excited\", \"excited\", \"exciting\", \"exciting\", \"exciting\", \"expectation\", \"experience\", \"experience\", \"experience\", \"experience\", \"eye\", \"eye\", \"facebook\", \"failure\", \"fall\", \"fan\", \"favourite\", \"fear\", \"fear\", \"feedback\", \"fellow\", \"female\", \"finance\", \"finance\", \"financial\", \"financial\", \"financial\", \"financial_service\", \"financial_time\", \"firm\", \"firm\", \"firm\", \"first\", \"first\", \"first\", \"first\", \"first\", \"first\", \"focused\", \"food\", \"food\", \"food\", \"forum\", \"founder\", \"founder\", \"framework\", \"free\", \"free\", \"free\", \"free\", \"free\", \"freedom\", \"friend\", \"friend\", \"fun\", \"funding\", \"funny\", \"future\", \"future\", \"future\", \"g\", \"game\", \"game\", \"genius\", \"global\", \"global\", \"global\", \"goal\", \"goal\", \"goal\", \"good\", \"good\", \"good\", \"good\", \"good\", \"google\", \"government\", \"government\", \"government\", \"grateful\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"growth\", \"growth\", \"guest\", \"guide\", \"habit\", \"hack\", \"hand\", \"hand\", \"hand\", \"hand\", \"happiness\", \"happy\", \"happy\", \"hard\", \"hard\", \"hard\", \"hard\", \"health\", \"healthcare\", \"healthcare\", \"high\", \"high\", \"high\", \"high\", \"high\", \"holiday\", \"home\", \"home\", \"home\", \"home\", \"honest\", \"hospital\", \"host\", \"hot\", \"hour\", \"hour\", \"hour\", \"human\", \"human\", \"human\", \"humanity\", \"idea\", \"idea\", \"idea\", \"idea\", \"ideal\", \"identity\", \"impact\", \"impact\", \"impact\", \"impact\", \"impact\", \"impact\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"important\", \"impossible\", \"inclusion\", \"incredible\", \"indian\", \"indistractable\", \"industry\", \"industry\", \"industry\", \"industry\", \"influence\", \"influencer\", \"innovation\", \"insight\", \"insight\", \"insight\", \"insightful\", \"inspiration\", \"inspiring\", \"insurance\", \"intention\", \"intentional\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"interesting\", \"internet\", \"investing\", \"investment\", \"investment\", \"investor\", \"investor\", \"investor\", \"iphone\", \"issue\", \"issue\", \"issue\", \"issue\", \"issue\", \"job\", \"job\", \"job\", \"journalism\", \"journalist\", \"journey\", \"joy\", \"key\", \"key\", \"key\", \"key\", \"key\", \"kindness\", \"language\", \"late\", \"late\", \"late\", \"late\", \"late\", \"late\", \"late\", \"late_article\", \"late_episode\", \"leader\", \"leader\", \"leader\", \"legacy\", \"lesson\", \"life\", \"life\", \"life\", \"link\", \"link\", \"link\", \"link\", \"link_comment\", \"linkedin\", \"linkedin\", \"list\", \"list\", \"little\", \"little\", \"livestream\", \"lockdown\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lot\", \"lucky\", \"machine\", \"machine\", \"magazine\", \"magic\", \"man\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"many\", \"market\", \"market\", \"market\", \"marketer\", \"marketing\", \"marketplace\", \"mask\", \"masterclass\", \"medical\", \"medical_futurist\", \"medicine\", \"meditation\", \"medium\", \"medium\", \"medium\", \"meeting\", \"meeting\", \"mental\", \"mentor\", \"message\", \"message\", \"message\", \"message\", \"method\", \"million\", \"minute\", \"minute\", \"mistake\", \"mobile\", \"money\", \"money\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"more\", \"morning\", \"morning\", \"morning\", \"most\", \"most\", \"most\", \"most\", \"most\", \"mother\", \"motivation\", \"movie\", \"much\", \"much\", \"much\", \"much\", \"name\", \"name\", \"name\", \"need\", \"need\", \"need\", \"need\", \"netflix\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new_episode\", \"news\", \"news\", \"newsletter\", \"next\", \"next\", \"next\", \"next\", \"next\", \"none\", \"number\", \"number\", \"number\", \"number\", \"offering\", \"okay\", \"opportunity\", \"opportunity\", \"opportunity\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"other\", \"own\", \"own\", \"own\", \"own\", \"pandemic\", \"pandemic\", \"pandemic\", \"pandemic\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"part\", \"passion\", \"passion\", \"past_year\", \"patient\", \"payment\", \"peace\", \"people\", \"people\", \"percent\", \"performance\", \"performance\", \"person\", \"person\", \"personal_brand\", \"personal_branding\", \"phone\", \"physician\", \"pitch\", \"planet\", \"platform\", \"platform\", \"platform\", \"platform\", \"play\", \"player\", \"pm\", \"podcast\", \"podcast\", \"policy\", \"policy\", \"policy\", \"popular\", \"population\", \"possibility\", \"post\", \"post\", \"post\", \"power\", \"power\", \"power\", \"pr\", \"presentation\", \"pressure\", \"pressure\", \"price\", \"price\", \"price\", \"priority\", \"priority\", \"privacy\", \"pro\", \"problem\", \"problem\", \"problem\", \"product\", \"product\", \"product_service\", \"production\", \"productive\", \"productivity\", \"profile\", \"profit\", \"program\", \"program\", \"proud\", \"proud\", \"psychology\", \"question\", \"question\", \"race\", \"radical\", \"rare\", \"read\", \"reading\", \"ready\", \"ready\", \"ready\", \"reason\", \"reason\", \"reason\", \"recommendation\", \"regulation\", \"relationship\", \"relationship\", \"relationship\", \"relevant\", \"remote\", \"report\", \"report\", \"report\", \"research\", \"research\", \"research\", \"result\", \"result\", \"result\", \"result\", \"retail\", \"retailer\", \"return\", \"return\", \"revenue\", \"review\", \"reward\", \"right\", \"right\", \"right\", \"right\", \"risk\", \"risk\", \"risk\", \"risk\", \"road\", \"robot\", \"role\", \"role\", \"role\", \"role\", \"round\", \"rule\", \"rule\", \"rule\", \"sale\", \"same\", \"same\", \"same\", \"same\", \"same\", \"schedule\", \"school\", \"science\", \"science\", \"science\", \"search\", \"secret\", \"segment\", \"series\", \"series\", \"service\", \"service\", \"service\", \"service\", \"show\", \"situation\", \"situation\", \"skill\", \"skill\", \"sleep\", \"small_business\", \"smart\", \"smart\", \"smartphone\", \"social_medium\", \"soft_skill\", \"solution\", \"solution\", \"solution\", \"source\", \"speaker\", \"speech\", \"spirit\", \"startup\", \"startup\", \"state\", \"state\", \"stock\", \"stock\", \"story\", \"story\", \"storytelling\", \"strategy\", \"strategy\", \"stress\", \"stress\", \"stress\", \"student\", \"study\", \"such\", \"such\", \"such\", \"such\", \"such\", \"support\", \"support\", \"sure\", \"sure\", \"sure\", \"sustainable\", \"system\", \"system\", \"system\", \"system\", \"tactic\", \"take\", \"talk\", \"talk\", \"task\", \"team\", \"team\", \"tech\", \"technique\", \"technology\", \"test\", \"testing\", \"thank\", \"thank\", \"thank\", \"thankful\", \"theme\", \"thing\", \"thing\", \"think\", \"thought\", \"thought\", \"thought\", \"thought\", \"thought\", \"thousand\", \"ticket\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tip\", \"tip\", \"title\", \"today\", \"today\", \"today\", \"today\", \"today\", \"today\", \"tomorrow\", \"tomorrow\", \"tool\", \"tool\", \"tool\", \"tool\", \"tool\", \"top\", \"top\", \"top\", \"top\", \"top\", \"topic\", \"topic\", \"topic\", \"trade\", \"transformation\", \"transunion\", \"travel\", \"treatment\", \"tree\", \"trend\", \"trend\", \"trend\", \"tropical_disease\", \"true\", \"true\", \"true\", \"tune\", \"tv\", \"uber\", \"uncertainty\", \"uncertainty\", \"uncomfortable\", \"understanding\", \"useful\", \"useful\", \"vaccine\", \"valuable\", \"value\", \"value\", \"value\", \"venture\", \"video\", \"video_credit\", \"video_game\", \"view\", \"view\", \"view\", \"virgin\", \"voice\", \"voice\", \"voice\", \"war\", \"way\", \"way\", \"way\", \"way\", \"way\", \"wealth\", \"week\", \"week\", \"week\", \"week\", \"week\", \"weekend\", \"weekend\", \"weekly\", \"well\", \"well\", \"well\", \"well\", \"well\", \"window\", \"wisdom\", \"woman\", \"word\", \"word\", \"word\", \"work\", \"work\", \"work\", \"worker\", \"world\", \"world\", \"world\", \"world\", \"world\", \"world\", \"writing\", \"writing\", \"wrong\", \"wrong\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year_old\", \"young\", \"zoom\", \"\\u202c\", \"\\u2705\", \"\\u2714\", \"\\u2764\", \"\\ud83c\\udf57\", \"\\ud83d\\udc47\", \"\\ud83d\\udda4\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 4, 7, 10, 9, 8, 2, 5, 3, 6]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el1406425577926794089102605968\", ldavis_el1406425577926794089102605968_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el1406425577926794089102605968\", ldavis_el1406425577926794089102605968_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.3.1/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el1406425577926794089102605968\", ldavis_el1406425577926794089102605968_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wait, what am I looking at again?**\n",
    "\n",
    "There are a lot of moving parts in the visualization. Here's a brief summary:\n",
    "\n",
    "* On the left, there is a plot of the \"distance\" between all of the topics (labeled as the Intertopic Distance Map)\n",
    "    * The plot is rendered in two dimensions according a multidimensional scaling (MDS) algorithm. Topics that are generally similar should be appear close together on the plot, while dissimilar topics should appear far apart.\n",
    "    * The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus.\n",
    "    * An individual topic may be selected for closer scrutiny by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left.\n",
    "\n",
    "* On the right, there is a bar chart showing top terms.\n",
    "    * When no topic is selected in the plot on the left, the bar chart shows the top-30 most \"salient\" terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics.\n",
    "    * When a particular topic is selected, the bar chart changes to show the top-30 most \"relevant\" terms for the selected topic. The relevance metric is controlled by the parameter , which can be adjusted with a slider above the bar chart.\n",
    "        * Setting the  parameter close to 1.0 (the default) will rank the terms solely according to their probability within the topic.\n",
    "        * Setting  close to 0.0 will rank the terms solely according to their \"distinctiveness\" or \"exclusivity\" within the topic — i.e., terms that occur only in this topic, and do not occur in other topics.\n",
    "        * Setting  to values between 0.0 and 1.0 will result in an intermediate ranking, weighting term probability and exclusivity accordingly.\n",
    "* Rolling the mouse over a term in the bar chart on the right will cause the topic circles to resize in the plot on the left, to show the strength of the relationship between the topics and the selected term.\n",
    "\n",
    "A more detailed explanation of the pyLDAvis visualization can be found here. Unfortunately, though the data used by gensim and pyLDAvis are the same, they don't use the same ID numbers for topics. If you need to match up topics in gensim's LdaMulticore object and pyLDAvis' visualization, you have to dig through the terms manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What about our us ?\n",
    "\n",
    "Our first decision to name topics are pretty right, except for topics 5 & 10 (on pyLDAvis). Indeed, looking at it more specifically (on gensim ID's where we named manually each topic) it seems that topics 3 & 4 (World new & Health,Covid) are strongly similar. It seems that these topics are relative to the political & health situation (pandemic, health care, security, ...) in USA. \n",
    "\n",
    "But because Topics are well separated in global, we will keep things like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have tested to create 9topics instead of 10, but it appears that topics are more relatalble than with 10.\n",
    "# Therefore we are not going to use 9 topics later, but we keep that in this notebook\n",
    "# If you want to run this code, make the following statement true\n",
    "if 1 == 1:\n",
    "\n",
    "    # Train the model on the corpus.\n",
    "    lda = LdaModel(corpus=bag_of_words,id2word=corpus_dictionary, num_topics=9,passes=100)\n",
    "    LDAvis_prepared = gensimvis.prepare(lda, bag_of_words,\n",
    "                                                corpus_dictionary)\n",
    "    pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reactions & Topics**\n",
    "\n",
    "Now we have defined our 10 topics, we will observe which topics get the most reactions !\n",
    "\n",
    "To do so, we will create a function based on what we have done previously that takes as entry a raw post and return the topic distribution.  \n",
    "We define a minimum topic frequency to not consider a certain topic if it is not relevant enough in the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_description(post, min_topic_freq = 0.1):\n",
    "\n",
    "    # First we remove hashtags with rm_hashtags\n",
    "    rm_hash_post = rm_hashtags(post)\n",
    "\n",
    "    # Lowercase the text\n",
    "    lowcase_post = rm_hash_post.lower()\n",
    "\n",
    "    # Clean it with rules we have defined previously\n",
    "    # Rules are : not punct - not whitespace - not stopword - Nouns & Adjectives only\n",
    "    # cleaned_post is here a list of tokens\n",
    "    parsed_post = nlp(lowcase_post)\n",
    "    cleaned_post = [token.lemma_ for token in parsed_post if all(rules(token))]\n",
    "\n",
    "    # Define 2-words phrases\n",
    "    # Return a list of phrases / tokens\n",
    "    bigram_post = bigram_model[cleaned_post]\n",
    "\n",
    "    # Create a bag_of_words representation\n",
    "    bow_post = corpus_dictionary.doc2bow(bigram_post)\n",
    "\n",
    "    # Create an LDA representation\n",
    "    lda_post = lda[bow_post]\n",
    "\n",
    "    # Sort with the most highly related topics first\n",
    "    # lda_post = sorted(lda_post, key=lambda freq : -freq)\n",
    "\n",
    "    for topic_number, freq in lda_post:\n",
    "        if freq < min_topic_freq:\n",
    "            continue\n",
    "        print ('{:20}{:.02f}'.format(topic_names[topic_number],\n",
    "                                round(freq, 3)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the imf predicts we will be entering the worst global recession since the 1930s, and are forecasting a downturn on par with the great depression. all of these negative predictions can lead to anxiety. what's going to be important is to manage that anxiety, and remember - there is hope. stay happy & healthy. \""
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = data[['ID','Content']]\n",
    "example_post = corpus[\"Content\"][18]\n",
    "example_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interview           0.22\n",
      "Money               0.72\n"
     ]
    }
   ],
   "source": [
    "lda_description(example_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each cleaned post, we define which topics are present.\n",
    "# It takes the form of a vector with the distribution of topics as values between 0 & 1\n",
    "\n",
    "# As previously, we need to stream each post into individual words\n",
    "# Then we have to transform these sequences/lists of words into bag_of_words (bow) thanks to the dictionary we have created previously\n",
    "# Let's use bigram_corpus we have created previously with 2-words phrases\n",
    "\n",
    "# We take posts after cleaning and modification, we ahve saved in bigram_corpus\n",
    "posts = pd.Series(bigram_corpus)\n",
    "\n",
    "#A list of posts, where each post is streamed into words\n",
    "streamed_posts = posts.apply(lambda post : post.split(' '))\n",
    "\n",
    "#Transform these sequences into bow.\n",
    "bow_posts = [corpus_dictionary.doc2bow(post) for post in streamed_posts]\n",
    "\n",
    "#Then we use the Lda model we trained previously\n",
    "post_topics = lda[bow_posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello, my name is"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = 'Hello, my name is'\n",
    "nlp(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hello, my name is"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(ex)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae290cde4634a6443381d55bbe6f1180de26a7de00f3f0eb338834ba70f07938"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
